[["index.html", "Analysis Using R Canadian Bioinformatics Workshop Welcome Meet your Faculty Pre-workshop Materials and Laptop Setup Instructions", " Analysis Using R Canadian Bioinformatics Workshop Instructors: Shraddha Pai, Chaitra Sarathy last modified 2023-06-27 Welcome Welcome to Analysis Using R 2023. Meet your Faculty Shraddha Pai Investigator I, OICR Assistant Professor, University of Toronto Dr. Pai integrates genomics and computational methods to advance precision medicine. Her previous work involves DNA methylome-based biomarker discovery in psychosis, and building machine learning algorithms for patient classification from multi-modal data. The Pai Lab at the Ontario Institute for Cancer Research focuses on biomarker discovery for detection, diagnosis and prognosis in brain cancers and other brain-related disorders. Chaitra Sarathy Scientific Associate Princess Margaret Cancer Centre University Health Network Dr. Sarathy is a computational biologist with experience in software development. During her PhD, she applied mathematical modelling, network analysis and multi-omics integration to study complex diseases. She has contributed to open-source toolboxes (openCOBRA) and developed softwares (EFMviz &amp; ComMet) to analyse genome-scale metabolic models. She currently works in the Bader lab’s MODiL team (Multi Omics Data Integration and Analysis) and with groups at PMCC, where she develops pipelines to analyse various omics data types and discover new drug targets in cancer. Delaram Pouyabahar PhD Candidate University of Toronto, Donnelly Centre Delaram is a Ph.D. candidate in Computational Biology at the Molecular Genetics department at UofT. She completed her undergraduate studies at the University of Tehran before joining the Bader lab in Toronto. Her research centers around the development of computational tools to explore sources of variation in single-cell transcriptomics maps, with a particular emphasis on liver biology and transplantation. Ian Cheong MSc. Candidate University of Toronto Ian is a Master’s level candidate in the Department of Medical Biophysics at the University of Toronto. His thesis work in the Pai lab involves analysis of single-cell transcriptomes to find the link between brain development and the development of childhood brain cancer.       Nia Hughes Program Manager Bioinformatics.ca nia.hughes@oicr.on.ca Nia is the Program Manager for Bioinformatics.ca, where she coordinates the Canadian Bioinformatics Workshop Series. Prior to starting at the OICR, she completed her M.Sc in Bioinformatics from the University of Guelph in 2020 before working there as a bioinformatician studying epigenetic and transcriptomic patterns across maize varieties. Pre-workshop Materials and Laptop Setup Instructions Laptop Setup Instructions A checklist to setup your laptop can be found here. Install these tools on your laptop before coming to the workshop: R (4.0+) Note: MacBook users with an Apple silicon chip (e.g., M1 or M2) should install the “arm64” version of R, while MacBook users with an Intel chip should install the regular (64-bit) version of R. You can check your laptop’s hardware specifications by clicking the Apple icon (top left corner) &gt; About This Mac and verifying whether the chip is Apple or Intel. Rstudio Make sure you have a robust internet browser such as Firefox, Safari or Chrome (not Internet Explorer). Make sure you have a PDF viewer (e.g. Adobe Acrobat, Preview or similar) or that you can read PDF files in your Web browser. R packages pkgList &lt;- c(&quot;tidyverse&quot;, &quot;clValid&quot;,&quot;rgl&quot;,&quot;RColorBrewer&quot;,&quot;corrplot&quot;,&quot;ClusterR&quot;, &quot;Rtsne&quot;,&quot;umap&quot;,&quot;BiocManager&quot;,&quot;mlbench&quot;,&quot;plotrix&quot;, &quot;factoextra&quot;) for (cur in pkgList){ message(sprintf(&quot;\\tChecking for %s ...&quot;, cur)) if (!requireNamespace(cur, quietly = TRUE)) install.packages(cur) } ## Checking for tidyverse ... ## Checking for clValid ... ## Checking for rgl ... ## Checking for RColorBrewer ... ## Checking for corrplot ... ## Checking for ClusterR ... ## Checking for Rtsne ... ## Checking for umap ... ## Checking for BiocManager ... ## Checking for mlbench ... ## Checking for plotrix ... ## Checking for factoextra ... if (!requireNamespace(&quot;edgeR&quot;, quietly = TRUE)) BiocManager::install(&quot;edgeR&quot;) if (!requireNamespace(&quot;curatedTCGAData&quot;, quietly = TRUE)) BiocManager::install(&quot;curatedTCGAData&quot;) We are going to use titanic.csv in Module 3. Download the dataset here and copy it into your working directory. "],["lecture-slides.html", "Lecture slides", " Lecture slides Module 1: Exploratory Data Analysis and Clustering Module 2: Dimensionality reduction for visualization and analysis Module 3: Generalized linear models Module 4: Multiple hypothesis testing with RNA-seq differential expression analysis "],["module-1-exploratory-data-analysis-and-clustering.html", "Module 1: Exploratory Data Analysis and Clustering Load mouse data RColorBrewer for colour palettes Correlations, distances, and clustering Hierarchical clustering K-means clustering Using clValid to determine number of clusters Exercise", " Module 1: Exploratory Data Analysis and Clustering The goal of this lab is to examine correlation structure in a sample transcriptomic dataset using clustering. Specifically, we will: Load and briefly explore transcriptomic measures from two mouse tissues Explore the correlation structure in the data using pairs() See the effect of running k-means and hierarchical clustering on the data Use clValid to find out what cluster number best separates groups We will explore missing data in detail in Module 3. Load mouse data For this exercise we are going to load a dataset built into the clValid package. This dataset measures 147 genes and expressed sequence tags in two developing mouse lineages: the neural crest cells and mesoderm-derived cells. There are three samples per group. Let’s load the data. suppressMessages(library(clValid)) data(mouse) Use str() to see what types of data the columns have: str(mouse) ## &#39;data.frame&#39;: 147 obs. of 8 variables: ## $ ID : Factor w/ 147 levels &quot;1415787_at&quot;,&quot;1415904_at&quot;,..: 111 88 93 74 138 103 46 114 112 24 ... ## $ M1 : num 4.71 3.87 2.88 5.33 5.37 ... ## $ M2 : num 4.53 4.05 3.38 5.5 4.55 ... ## $ M3 : num 4.33 3.47 3.24 5.63 5.7 ... ## $ NC1: num 5.57 5 3.88 6.8 6.41 ... ## $ NC2: num 6.92 5.06 4.46 6.54 6.31 ... ## $ NC3: num 7.35 5.18 4.85 6.62 6.2 ... ## $ FC : Factor w/ 9 levels &quot;ECM/Receptors&quot;,..: 3 8 6 6 1 3 1 6 5 6 ... Another command is head(): head(mouse) ## ID M1 M2 M3 NC1 NC2 NC3 ## 1 1448995_at 4.706812 4.528291 4.325836 5.568435 6.915079 7.353144 ## 2 1436392_s_at 3.867962 4.052354 3.474651 4.995836 5.056199 5.183585 ## 3 1437434_a_at 2.875112 3.379619 3.239800 3.877053 4.459629 4.850978 ## 4 1428922_at 5.326943 5.498930 5.629814 6.795194 6.535522 6.622577 ## 5 1452671_s_at 5.370125 4.546810 5.704810 6.407555 6.310487 6.195847 ## 6 1448147_at 3.471347 4.129992 3.964431 4.474737 5.185631 5.177967 ## FC ## 1 Growth/Differentiation ## 2 Transcription factor ## 3 Miscellaneous ## 4 Miscellaneous ## 5 ECM/Receptors ## 6 Growth/Differentiation Summary provides useful information about the distribution of variables. Note that FC has categorical variables: summary(mouse) ## ID M1 M2 M3 ## 1415787_at: 1 Min. :2.352 Min. :2.139 Min. :2.500 ## 1415904_at: 1 1st Qu.:4.188 1st Qu.:4.151 1st Qu.:4.207 ## 1415993_at: 1 Median :4.994 Median :5.043 Median :5.054 ## 1416164_at: 1 Mean :5.166 Mean :5.140 Mean :5.231 ## 1416181_at: 1 3rd Qu.:6.147 3rd Qu.:6.015 3rd Qu.:6.129 ## 1416221_at: 1 Max. :9.282 Max. :9.273 Max. :9.228 ## (Other) :141 ## NC1 NC2 NC3 ## Min. :2.100 Min. :1.996 Min. :2.125 ## 1st Qu.:4.174 1st Qu.:4.136 1st Qu.:4.293 ## Median :4.996 Median :5.056 Median :4.974 ## Mean :5.120 Mean :5.134 Mean :5.118 ## 3rd Qu.:5.860 3rd Qu.:5.920 3rd Qu.:5.826 ## Max. :8.905 Max. :8.954 Max. :9.251 ## ## FC ## EST :31 ## Transcription factor :28 ## Miscellaneous :25 ## ECM/Receptors :16 ## Growth/Differentiation:16 ## Unknown :10 ## (Other) :21 What are the values in FC? table(mouse$FC) ## ## ECM/Receptors EST Growth/Differentiation ## 16 31 16 ## Kinases/Phosphatases Metabolism Miscellaneous ## 7 8 25 ## Stress-induced Transcription factor Unknown ## 6 28 10 Let’s use pairs() to look at pairwise scatterplots of the expression data in a single plot. We need to subset the columns with the expression data first: mouse_exp = mouse[,c(&quot;M1&quot;,&quot;M2&quot;,&quot;M3&quot;,&quot;NC1&quot;,&quot;NC2&quot;,&quot;NC3&quot;)] pairs(mouse_exp) RColorBrewer for colour palettes Visualization often requires assigning colours to categories of interest (e.g., two different tissue types, eight levels of doses). RColorBrewer is a good package to pick a colour scheme for your project, as palettes are derived using colour science: Let’s look at all the palettes available. library(RColorBrewer) display.brewer.all() We’re going to use RColorBrewer to assign colours to our heatmap below. Correlations, distances, and clustering Let’s look at sample correlation matrix. This should be a square matrix, equal to the number of samples. We have six samples, so the correlation matrix should be 6x6. library(corrplot) mouse_cor &lt;- cor(mouse_exp) dim(mouse_cor) ## [1] 6 6 round(mouse_cor,2) ## M1 M2 M3 NC1 NC2 NC3 ## M1 1.00 0.98 0.98 0.84 0.81 0.78 ## M2 0.98 1.00 0.95 0.84 0.81 0.79 ## M3 0.98 0.95 1.00 0.82 0.78 0.75 ## NC1 0.84 0.84 0.82 1.00 0.97 0.95 ## NC2 0.81 0.81 0.78 0.97 1.00 0.99 ## NC3 0.78 0.79 0.75 0.95 0.99 1.00 corrplot(mouse_cor, method=&quot;color&quot;) Which samples appear to be best correlated with each other? Which samples don’t appear to be as well correlated with each other? Hierarchical clustering Hierarchical clustering requires distances between samples. Let’s use dist() to compute these distances, and hclust() to generate the hierarchical clustering object. Different values for method can produce different results. In our experience complete or ward.D2 produce stable results. d &lt;- dist(log(mouse_exp)) h &lt;- hclust(d,method=&quot;ward.D2&quot;) plot(h) Can you guess how many clusters could best fit the data? Now let’s add a heatmap to this dendrogram, so we can see the values of genes in each cluster. For this we will use the heatmap() function, which requires assigned cluster labels to each sample, and a dendrogram-generating function. So let’s create these. First we get cluster assignments by “cutting” the dendrogram for two clusters (something we expect from our experimental design). We use cutree() for this. Then assign colours based on cluster assignment. h2 &lt;- cutree(h, k = 2) h2cols &lt;- c(&quot;orangered&quot;,&quot;blue&quot;)[h2] Now define a hierarchical clustering function: hclust_fun &lt;- function(x){ f &lt;- hclust(x, method = &quot;ward.D2&quot;); return(f) } And finally supply the two to the heatmap() function: heatmap( as.matrix(mouse_exp), col = brewer.pal(&quot;Blues&quot;,n=8), hclustfun = hclust_fun, RowSideColors = h2cols, # use colours from cutree call above ColSideColors = c( rep(&quot;darkgreen&quot;,3), rep(&quot;deeppink2&quot;,3) ) ) Note that the two colours are completely divided (i.e., there is no interspersed red and blue). This is a good sign that the clustering function used the identical clustering method, here, ward.D2, to cluster the samples. Compare this result to what happens if we try the same function call without clustering: heatmap( as.matrix(mouse_exp), col = brewer.pal(&quot;Blues&quot;,n=8), RowSideColors = h2cols, # use colours from cutree call above ColSideColors = c( rep(&quot;darkgreen&quot;,3), rep(&quot;deeppink2&quot;,3) ) ) K-means clustering Let’s try using k-means clustering, asking for three clusters: kclust &lt;- kmeans( mouse_exp, centers = 3 ) kclust ## K-means clustering with 3 clusters of sizes 22, 64, 61 ## ## Cluster means: ## M1 M2 M3 NC1 NC2 NC3 ## 1 7.416536 7.406216 7.414799 7.548674 7.534414 7.520608 ## 2 5.553148 5.499583 5.642404 5.426931 5.435363 5.353342 ## 3 3.947440 3.946048 4.012209 3.922949 3.950984 4.004362 ## ## Clustering vector: ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## 2 3 3 2 2 3 2 2 2 3 1 3 2 1 3 2 1 ## 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## 1 3 2 2 2 3 3 3 3 1 2 3 2 2 1 3 2 ## 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## 2 3 3 3 2 3 2 1 1 3 3 1 3 3 3 3 2 ## 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## 3 1 1 2 2 3 3 2 3 3 1 2 3 2 3 2 1 ## 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## 2 3 2 2 2 2 2 1 3 3 2 1 2 1 3 2 2 ## 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 ## 3 1 2 3 2 3 3 3 3 3 3 3 2 3 2 3 2 ## 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 ## 3 3 3 3 1 2 3 2 2 3 2 2 2 2 1 1 3 ## 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 ## 2 2 2 1 3 2 2 2 3 2 2 2 2 2 3 3 3 ## 137 138 139 140 141 142 143 144 145 146 147 ## 3 3 3 2 2 3 2 1 2 2 2 ## ## Within cluster sum of squares by cluster: ## [1] 81.44264 166.24343 193.59229 ## (between_SS / total_SS = 74.3 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; Using clValid to determine number of clusters Use the clValid() function to validate clusters using the: Dunn index, silhouette scores, and connectivity validation_data &lt;- clValid( mouse_exp, 2:6, # num. clusters to evaluate clMethods = c(&quot;hier&quot;,&quot;kmeans&quot;), # methods to eval. validation = &quot;internal&quot; ) Let’s look at the results: summary(validation_data) ## ## Clustering Methods: ## hierarchical kmeans ## ## Cluster sizes: ## 2 3 4 5 6 ## ## Validation Measures: ## 2 3 4 5 6 ## ## hierarchical Connectivity 5.3270 14.2528 20.7520 27.0726 30.6194 ## Dunn 0.1291 0.0788 0.0857 0.0899 0.0899 ## Silhouette 0.5133 0.4195 0.3700 0.3343 0.3233 ## kmeans Connectivity 13.2548 17.6651 37.3980 43.2655 50.6095 ## Dunn 0.0464 0.0873 0.0777 0.0815 0.0703 ## Silhouette 0.4571 0.4182 0.3615 0.3367 0.3207 ## ## Optimal Scores: ## ## Score Method Clusters ## Connectivity 5.3270 hierarchical 2 ## Dunn 0.1291 hierarchical 2 ## Silhouette 0.5133 hierarchical 2 All measures of clustering consistently indicate that two clusters best fit the data. Now let’s cluster: d &lt;- dist(log(mouse_exp)) h &lt;- hclust(d,method=&quot;ward.D2&quot;) cluster_ids &lt;- cutree(h, k = 2) clust_colors &lt;- c(&quot;dodgerblue&quot;,&quot;orangered&quot;)[cluster_ids] heatmap( as.matrix(mouse_exp), col = brewer.pal(&quot;Blues&quot;,n=8), hclustfun = hclust_fun, RowSideColors = clust_colors, # kmeans labels ColSideColors = c( rep(&quot;darkgreen&quot;,3), rep(&quot;deeppink2&quot;,3) ) ) Exercise For your exercise, try the following: Load the MASS package using: library(MASS) Import crabs dataset using: data(crabs) Learn about this dataset using: ?crabs Extract the numeric columns describing the crab measurements (“FL”, “RW”, “CL”, “CW”, “BD”) Cluster the numeric columns using your method of choice Plot and color your data by clusters, by species (sp), and sex Do your clusters seem to separate these groups in the same way? "],["module-2-dimensionality-reduction.html", "Module 2: Dimensionality reduction Principal Component Analysis t-Distributed Stochastic Neighbor Embedding (t-SNE) Uniform Manifold Approximation and Projection (UMAP) Exercise", " Module 2: Dimensionality reduction The goal of this lab is to learn how to reduce the dimension of your dataset. We will learn three different methods commonly used for dimension reduction: Principal Component Analysis t-stochastic Neighbour Embedding (tSNE) Uniform Manifold Approximation (UMAP) Principal Component Analysis Let’s start with PCA. PCA is commonly used as one step in a series of analyses. The goal of PCA is to explain most of the variability in the data with a smaller number of variables than the original data set. You can use PCA to explain the variability in your data using fewer variables. Typically, it is useful to identify outliers and determine if there’s batch effect in your data. Data: We will use the dataset that we used for exploratory analysis in Module 1. Load the mouse data. library(clValid) data(&quot;mouse&quot;) mouse_exp = mouse[,c(&quot;M1&quot;,&quot;M2&quot;,&quot;M3&quot;,&quot;NC1&quot;,&quot;NC2&quot;,&quot;NC3&quot;)] Step 1. Preparing Our Data It is important to make sure that all the variables in your dataset are on the same scale to ensure they are comparable. So, let us check if that is the case with our dataset. To do that, we will first compute the means and variances of each variable using apply(). apply(mouse_exp, 2, mean) ## M1 M2 M3 NC1 NC2 NC3 ## 5.165708 5.140265 5.231185 5.120369 5.133540 5.117914 apply(mouse_exp, 2, var) ## M1 M2 M3 NC1 NC2 NC3 ## 1.858482 1.848090 1.869578 2.005517 2.080473 2.083073 As you can see, the means and variances for all the six variables are almost the same and on the same scale, which is great! However, keep in mind that, the variables need not always be on the same scale in other non-omics datasets. PCA is influenced by the magnitude of each variable. So, it is important to include a scaling step during data preparation. Ideally, it is great to have variables centered at zero for PCA because it makes comparing each principal component to the mean straightforward. Scaling can be done either using scale(). Step 2. Apply PCA Since our variables are on the same scale, we can directly apply PCA using prcomp(). pc_out &lt;- prcomp(mouse_exp) The output of prcomp() is a list. Examine the internal structure of pc_out. str(pc_out) ## List of 5 ## $ sdev : num [1:6] 3.236 1.025 0.323 0.29 0.139 ... ## $ rotation: num [1:6, 1:6] 0.398 0.396 0.392 0.421 0.425 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:6] &quot;M1&quot; &quot;M2&quot; &quot;M3&quot; &quot;NC1&quot; ... ## .. ..$ : chr [1:6] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ... ## $ center : Named num [1:6] 5.17 5.14 5.23 5.12 5.13 ... ## ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;M1&quot; &quot;M2&quot; &quot;M3&quot; &quot;NC1&quot; ... ## $ scale : logi FALSE ## $ x : num [1:147, 1:6] 1.1 -1.69 -3.31 2.29 1.52 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:147] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:6] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ... ## - attr(*, &quot;class&quot;)= chr &quot;prcomp&quot; The output of prcomp() contains five elements sdev, rotation, center, scale and x. Let us examine what each looks like. pc_out$sdev ## [1] 3.2360215 1.0252515 0.3229312 0.2898690 0.1385061 0.1214254 sddev gives standard deviation (used for computing variance explained). We will see how in sections below. pc_out$rotation ## PC1 PC2 PC3 PC4 PC5 ## M1 0.3979811 -0.4224723 0.0227626352 -0.05836779 0.7021505 ## M2 0.3958871 -0.3727297 -0.6856494932 0.33963222 -0.2891868 ## M3 0.3919059 -0.4539621 0.5159998020 -0.39025307 -0.4118063 ## NC1 0.4212931 0.2885355 0.4257422036 0.68093993 -0.1706376 ## NC2 0.4246875 0.4150626 -0.0001226513 -0.07802616 0.4039780 ## NC3 0.4164696 0.4700834 -0.2861020801 -0.50909570 -0.2479028 ## PC6 ## M1 -0.4076615 ## M2 0.1877542 ## M3 0.2284525 ## NC1 -0.2553559 ## NC2 0.6914313 ## NC3 -0.4506507 After PCA, the observations are expressed in new axes and the loadings are provided in pc_out$rotation. Each column of pc_out$rotation contains the corresponding principal component loading vector. We see that there are six distinct principal components, as indicated by column names of pc_out$rotation. pc_out$center ## M1 M2 M3 NC1 NC2 NC3 ## 5.165708 5.140265 5.231185 5.120369 5.133540 5.117914 pc_out$scale ## [1] FALSE The center and scale elements correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA. # See the principal components dim(pc_out$x) ## [1] 147 6 head(pc_out$x) ## PC1 PC2 PC3 PC4 PC5 PC6 ## 1 1.096553 2.752444 -0.5069679 -0.7995930 0.3167151 -0.02456804 ## 2 -1.693525 1.713996 -0.2617839 0.2795517 0.1004742 -0.12777366 ## 3 -3.310239 1.764034 -0.3253869 -0.3452761 -0.2730373 0.12010215 ## 4 2.290027 1.389709 0.2458285 0.2218832 -0.2470962 -0.04370012 ## 5 1.523073 1.286454 0.8954116 -0.1624291 0.1087050 -0.08723939 ## 6 -1.795594 1.531001 -0.2915804 -0.2241446 -0.2595562 0.38547010 Let’s now see the summary of the analysis using the summary() function! summary(pc_out) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 3.2360 1.0253 0.32293 0.28987 0.13851 0.12143 ## Proportion of Variance 0.8916 0.0895 0.00888 0.00715 0.00163 0.00126 ## Cumulative Proportion 0.8916 0.9811 0.98996 0.99711 0.99874 1.00000 The first row gives the Standard deviation of each component, which is the same as the result of pc_out$sdev. The second row, Proportion of Variance, shows the percentage of explained variance, also obtained as variance/sum(variance) where variance is the square of sdev. Compute variance pc_out$sdev^2 / sum(pc_out$sdev^2) ## [1] 0.891583286 0.089495245 0.008878899 0.007153899 0.001633341 ## [6] 0.001255331 From the second row you can see that the first principal component explains over 89.15% of the total variance (Note: multiply each number by 100 to get the percentages). The second principal component explains 8.9% of the variance, and the amount of variance explained reduces further down with each component. Finally, the last row, Cumulative Proportion, calculates the cumulative sum of the second row. Now, let’s have some fun with visualising the results of PCA. Step 3. Visualisation of PCA results A. Scree plot We can visualize the percentage of explained variance per principal component by using what is called a scree plot. We will call the fviz_eig() function of the factoextra package for the application. You may need to install the package using install.packages(\"factoextra\"). library(factoextra) fviz_eig(pc_out, addlabels = TRUE) The x-axis shows the PCs and the y-axis shows the percentage of variance explained that we saw above. Percentages are listed on top of the bars. It’s common to see that the first few principal components explain the major amount of variance. Scree plot can also be used to decide the number of components to keep for rest of your analysis. One of the ways is using the elbow rule. This method is about looking for the “elbow” shape on the curve and retaining all components before the point where the curve flattens out. Here, the elbow appears to occur at the second principal component. Note that we will NOT remove any components for the current analysis since our goal is to understand how PCA can be used to identify batch effect in the data. B. Scatter plot After a PCA, the observations are expressed in principal component scores (as we saw above in pc_out$rotation). So, it is important to visualize the observations along the new axes (principal components) how observations have been transformed and to understand the relations in the dataset. This can be achieved by drawing a scatterplot. To do so, first, we need to extract and the principal component scores in pc_out$rotation, and then we will store them in a data frame called PC. PC &lt;- as.data.frame(pc_out$rotation) Plot the first two principal components as follows: plot(x = PC$PC1, y = PC$PC2, pch = 19, xlab=&quot;PC1&quot;, ylab=&quot;PC2&quot;) We see six points in two different groups. The points correspond to six samples. But, we don’t know what group/condition they belong to. That can be done by adding sample-related information to the data.frame PC (such as cell type, treatment type, batch they were processed etc) as new variables. Here we will add the sample names and the cell types. PC$sample &lt;- factor(rownames(PC)) PC$cells &lt;- factor(c(rep(&quot;mesoderm&quot;, 3), rep(&quot;neural_crest&quot;, 3))) Plot the scatterplot again and now, colour the points by cell type. Then, add sample names and legend. plot(x = PC$PC1, y = PC$PC2, col = PC$cells, pch = 19, xlab=&quot;PC1&quot;, ylab=&quot;PC2&quot;) text(x= PC$PC1, y = PC$PC2-0.02, labels = PC$sample) legend(&quot;bottomright&quot;, legend = levels(PC$cells), col = seq_along(levels(PC$cells)), pch = 19) Samples from each cell type are closer together on the scatter plot. If the batch information is available, it can also be used to colour the scatterplot. Ideally, samples from different conditions should cluster together, irrespective of the batch they were processed in. You can also plot other PCs such as PC2 vs PC3 by changing the x and y variables above. Another way to plot all PCs is using pairs() pairs(PC[,1:6], col=c(&quot;black&quot;,&quot;red&quot;)[PC$cells], pch=16) C. Biplot Another useful plots to understand the results are biplots. We will use the fviz_pca_biplot() function of the factoextra package. We will set label=“var” argument to label the variables. fviz_pca_biplot(pc_out, label = &quot;var&quot;) The axes show the principal component scores, and the vectors are the loading vectors, whose components are in the magnitudes of the loadings. Vectors indicate that samples from each cell type are closer together. t-Distributed Stochastic Neighbor Embedding (t-SNE) t-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. There are several packages that have implemented t-SNE. Here we are going to use the package tsne and the function tsne. Let’s run the t-SNE algorithm on the mouse and generate a t-SNE plot. library(tsne) mouse_tsne = tsne(mouse_exp) ## sigma summary: Min. : 0.509576105979237 |1st Qu. : 0.602616695625048 |Median : 0.634355559064648 |Mean : 0.631906807295978 |3rd Qu. : 0.663243612077724 |Max. : 0.762781922237503 | ## Epoch: Iteration #100 error is: 14.4391014334452 ## Epoch: Iteration #200 error is: 0.557856687210517 ## Epoch: Iteration #300 error is: 0.550149623868335 ## Epoch: Iteration #400 error is: 0.549702248698221 ## Epoch: Iteration #500 error is: 0.549702137036316 ## Epoch: Iteration #600 error is: 0.549702136808388 ## Epoch: Iteration #700 error is: 0.549702136807935 ## Epoch: Iteration #800 error is: 0.549702136807934 ## Epoch: Iteration #900 error is: 0.549702136807934 ## Epoch: Iteration #1000 error is: 0.549702136807934 library(RColorBrewer) my_cols_vec = brewer.pal(&quot;Set3&quot;,n = 9) plot(mouse_tsne, pch=16, col=my_cols_vec) # legend(&quot;topright&quot;, # legend = levels(mouse$FC), # col = my_cols_vec, # pch = 19) The tsne function has a parameter called perplexity which determines how to balance attention to neighborhood vs global structure. Default value is 30 which was used above. Set perplexity to 10, 20, 50, 100 and rerun tsne. Then visualise each result. mouse_tsne10 = tsne(mouse_exp,perplexity = 10) ## sigma summary: Min. : 0.334382646722384 |1st Qu. : 0.469002340728261 |Median : 0.504416116417653 |Mean : 0.504908740448725 |3rd Qu. : 0.545529972148454 |Max. : 0.648212409348644 | ## Epoch: Iteration #100 error is: 16.379082128038 ## Epoch: Iteration #200 error is: 0.772522398608794 ## Epoch: Iteration #300 error is: 0.717293971460226 ## Epoch: Iteration #400 error is: 2.05518681337245 ## Epoch: Iteration #500 error is: 1.65737207153557 ## Epoch: Iteration #600 error is: 1.39694862685831 ## Epoch: Iteration #700 error is: 1.26428391598378 ## Epoch: Iteration #800 error is: 1.18417738128737 ## Epoch: Iteration #900 error is: 1.10837835683022 ## Epoch: Iteration #1000 error is: 0.997233646685276 mouse_tsne20 = tsne(mouse_exp,perplexity = 20) ## sigma summary: Min. : 0.439896310102185 |1st Qu. : 0.547649117268216 |Median : 0.57999061558421 |Mean : 0.58021829284207 |3rd Qu. : 0.610637188011214 |Max. : 0.716269583885669 | ## Epoch: Iteration #100 error is: 15.3707414606571 ## Epoch: Iteration #200 error is: 0.700393219674423 ## Epoch: Iteration #300 error is: 0.624475783612349 ## Epoch: Iteration #400 error is: 0.590554451471992 ## Epoch: Iteration #500 error is: 0.585672480578968 ## Epoch: Iteration #600 error is: 0.584125331617327 ## Epoch: Iteration #700 error is: 0.583611105575618 ## Epoch: Iteration #800 error is: 0.583523062205254 ## Epoch: Iteration #900 error is: 0.583461290818503 ## Epoch: Iteration #1000 error is: 0.583410434058188 mouse_tsne50 = tsne(mouse_exp,perplexity = 50) ## sigma summary: Min. : 0.606839561720626 |1st Qu. : 0.68623338964489 |Median : 0.717382406583852 |Mean : 0.717069876829413 |3rd Qu. : 0.751004538772764 |Max. : 0.83908723113191 | ## Epoch: Iteration #100 error is: 12.8888201946862 ## Epoch: Iteration #200 error is: 0.491826603705988 ## Epoch: Iteration #300 error is: 0.478265588613178 ## Epoch: Iteration #400 error is: 0.478265524748615 ## Epoch: Iteration #500 error is: 0.478265524735605 ## Epoch: Iteration #600 error is: 0.478265524735558 ## Epoch: Iteration #700 error is: 0.478265524735553 ## Epoch: Iteration #800 error is: 0.478265524735553 ## Epoch: Iteration #900 error is: 0.478265524735553 ## Epoch: Iteration #1000 error is: 0.478265524735553 mouse_tsne100 = tsne(mouse_exp,perplexity = 100) ## sigma summary: Min. : 0.845838574587344 |1st Qu. : 0.924241978298709 |Median : 0.962433219707791 |Mean : 0.960666073206154 |3rd Qu. : 0.996456132520977 |Max. : 1.06111767663233 | ## Epoch: Iteration #100 error is: 11.64496048872 ## Epoch: Iteration #200 error is: 0.18465963284714 ## Epoch: Iteration #300 error is: 0.184659632847139 ## Epoch: Iteration #400 error is: 0.184659632847139 ## Epoch: Iteration #500 error is: 0.184659632847139 ## Epoch: Iteration #600 error is: 0.184659632847139 ## Epoch: Iteration #700 error is: 0.184659632847139 ## Epoch: Iteration #800 error is: 0.184659632847139 ## Epoch: Iteration #900 error is: 0.184659632847139 ## Epoch: Iteration #1000 error is: 0.184659632847139 par(mfrow=c(2,2)) plot(mouse_tsne10[,1], mouse_tsne10[,2], main = &quot;Perplexity = 10&quot;, col = my_cols_vec, pch=16) plot(mouse_tsne20[,1], mouse_tsne20[,2], main = &quot;Perplexity = 20&quot;, col = my_cols_vec, pch=16) plot(mouse_tsne50[,1], mouse_tsne50[,2], main = &quot;Perplexity = 50&quot;, col = my_cols_vec, pch=16) plot(mouse_tsne100[,1], mouse_tsne100[,2], main = &quot;Perplexity = 100&quot;, col = my_cols_vec, pch=16) Higher perplexity leads to higher spread in your data. Uniform Manifold Approximation and Projection (UMAP) UMAP is another dimension reduction method and it uses similar neighborhood approach as t-SNE except uses Riemannian geometry. Here we are going to use the package umap and the function umap. Let’s apply UMAP on the mouse and generate a UMAP plot. ## umap library(umap) mouse_umap = umap(mouse_exp) str(mouse_umap) ## List of 4 ## $ layout: num [1:147, 1:2] 3.23 -0.75 -2.35 3.9 3.96 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:147] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : NULL ## $ data : num [1:147, 1:6] 4.71 3.87 2.88 5.33 5.37 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:147] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:6] &quot;M1&quot; &quot;M2&quot; &quot;M3&quot; &quot;NC1&quot; ... ## $ knn :List of 2 ## ..$ indexes : int [1:147, 1:15] 1 2 3 4 5 6 7 8 9 10 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:147] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : NULL ## ..$ distances: num [1:147, 1:15] 0 0 0 0 0 0 0 0 0 0 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:147] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : NULL ## ..- attr(*, &quot;class&quot;)= chr &quot;umap.knn&quot; ## $ config:List of 24 ## ..$ n_neighbors : int 15 ## ..$ n_components : int 2 ## ..$ metric : chr &quot;euclidean&quot; ## ..$ n_epochs : int 200 ## ..$ input : chr &quot;data&quot; ## ..$ init : chr &quot;spectral&quot; ## ..$ min_dist : num 0.1 ## ..$ set_op_mix_ratio : num 1 ## ..$ local_connectivity : num 1 ## ..$ bandwidth : num 1 ## ..$ alpha : num 1 ## ..$ gamma : num 1 ## ..$ negative_sample_rate: int 5 ## ..$ a : num 1.58 ## ..$ b : num 0.895 ## ..$ spread : num 1 ## ..$ random_state : int 57662319 ## ..$ transform_state : int NA ## ..$ knn : logi NA ## ..$ knn_repeats : num 1 ## ..$ verbose : logi FALSE ## ..$ umap_learn_args : logi NA ## ..$ method : chr &quot;naive&quot; ## ..$ metric.function :function (m, origin, targets) ## ..- attr(*, &quot;class&quot;)= chr &quot;umap.config&quot; ## - attr(*, &quot;class&quot;)= chr &quot;umap&quot; par(mfrow=c(1,1)) plot(mouse_umap$layout[,1], mouse_umap$layout[,2], col = my_cols_vec, pch = 19) # legend(&quot;topright&quot;, # legend = levels(mouse$FC), # col = my_cols_vec, # pch = 19) Exercise For your exercise, try the following: Return to your crabs data Compute the principle components (PCs) for the numeric columns Plot these PCs and color them by species (“sp”) and sex Now compute 2 t-SNE components for these data and color by species and sex Finally compute 2 UMAP components for these data and color by species and sex Do any of these dimensionality reduction methods seem to segregate sex/species groups? "],["module-2-exercise.html", "Module 2 exercise", " Module 2 exercise PCA c_pcs = prcomp(crabs_meas) Plot PC projections (embeddings). pairs(c_pcs$x, col = c(&quot;orchid&quot;,&quot;forestgreen&quot;)[factor(crabs$sp)]) pairs(c_pcs$x, col = c(&quot;orchid&quot;,&quot;forestgreen&quot;)[factor(crabs$sex)]) tSNE: library(tsne) c_tsne10 = tsne(crabs_meas,perplexity = 10) ## sigma summary: Min. : 0.295392306171995 |1st Qu. : 0.424864940106807 |Median : 0.475900590252246 |Mean : 0.477263744443299 |3rd Qu. : 0.522810659014478 |Max. : 0.672971536327323 | ## Epoch: Iteration #100 error is: 14.5827253986851 ## Epoch: Iteration #200 error is: 0.450471032007349 ## Epoch: Iteration #300 error is: 0.419968799797693 ## Epoch: Iteration #400 error is: 0.399750141057301 ## Epoch: Iteration #500 error is: 0.395712252196237 ## Epoch: Iteration #600 error is: 0.393453242910054 ## Epoch: Iteration #700 error is: 0.391788961563803 ## Epoch: Iteration #800 error is: 0.390543082530683 ## Epoch: Iteration #900 error is: 0.389652807032385 ## Epoch: Iteration #1000 error is: 0.388835593514429 c_tsne20 = tsne(crabs_meas,perplexity = 20) ## sigma summary: Min. : 0.42069998064187 |1st Qu. : 0.505494820242659 |Median : 0.550282641638609 |Mean : 0.553782538032253 |3rd Qu. : 0.597446288884567 |Max. : 0.737568418500652 | ## Epoch: Iteration #100 error is: 13.8829552986345 ## Epoch: Iteration #200 error is: 0.433902858376159 ## Epoch: Iteration #300 error is: 0.376570037035713 ## Epoch: Iteration #400 error is: 0.37121080497109 ## Epoch: Iteration #500 error is: 0.370250941736815 ## Epoch: Iteration #600 error is: 0.3697929375522 ## Epoch: Iteration #700 error is: 0.369510455085253 ## Epoch: Iteration #800 error is: 0.369324716206597 ## Epoch: Iteration #900 error is: 0.369186170593725 ## Epoch: Iteration #1000 error is: 0.369079701440471 c_tsne50 = tsne(crabs_meas,perplexity = 50) ## sigma summary: Min. : 0.539839363698465 |1st Qu. : 0.634067694694373 |Median : 0.675230651916411 |Mean : 0.676426601512199 |3rd Qu. : 0.712708887622463 |Max. : 0.85041386579969 | ## Epoch: Iteration #100 error is: 12.9993554632573 ## Epoch: Iteration #200 error is: 0.309203014372782 ## Epoch: Iteration #300 error is: 0.306739993958062 ## Epoch: Iteration #400 error is: 0.297119265067537 ## Epoch: Iteration #500 error is: 0.29710452130143 ## Epoch: Iteration #600 error is: 0.297104521249994 ## Epoch: Iteration #700 error is: 0.297104521249994 ## Epoch: Iteration #800 error is: 0.297104521249994 ## Epoch: Iteration #900 error is: 0.297104521249994 ## Epoch: Iteration #1000 error is: 0.297104521249994 c_tsne100 = tsne(crabs_meas,perplexity = 100) ## sigma summary: Min. : 0.689338665294285 |1st Qu. : 0.801156853023062 |Median : 0.838030059692607 |Mean : 0.83585263946599 |3rd Qu. : 0.869043547272454 |Max. : 1.00462478171883 | ## Epoch: Iteration #100 error is: 10.4857987482431 ## Epoch: Iteration #200 error is: 0.24447561920575 ## Epoch: Iteration #300 error is: 0.244475527386451 ## Epoch: Iteration #400 error is: 0.244475527386425 ## Epoch: Iteration #500 error is: 0.244475527386425 ## Epoch: Iteration #600 error is: 0.244475527386425 ## Epoch: Iteration #700 error is: 0.244475527386425 ## Epoch: Iteration #800 error is: 0.244475527386425 ## Epoch: Iteration #900 error is: 0.244475527386425 ## Epoch: Iteration #1000 error is: 0.244475527386425 sex_cols = c(“orchid”,“forestgreen”)[factor(crabs$sex)] Color-code tSNE plot by species, try various perplexity levels: species_cols = c(&quot;orchid&quot;,&quot;forestgreen&quot;)[factor(crabs$sp)] par(mfrow=c(2,2)) plot(c_tsne10[,1], c_tsne10[,2], main = &quot;Perplexity = 10&quot;, col = species_cols) plot(c_tsne20[,1], c_tsne20[,2], main = &quot;Perplexity = 20&quot;, col = species_cols) plot(c_tsne50[,1], c_tsne50[,2], main = &quot;Perplexity = 50&quot;, col = species_cols) plot(c_tsne100[,1], c_tsne100[,2], main = &quot;Perplexity = 100&quot;, col = species_cols) Now do the same, but colour-code for sex: sex_cols = c(&quot;orchid&quot;,&quot;forestgreen&quot;)[factor(crabs$sex)] par(mfrow=c(2,2)) plot(c_tsne10[,1], c_tsne10[,2], main = &quot;Perplexity = 10&quot;, col = sex_cols) plot(c_tsne20[,1], c_tsne20[,2], main = &quot;Perplexity = 20&quot;, col = sex_cols) plot(c_tsne50[,1], c_tsne50[,2], main = &quot;Perplexity = 50&quot;, col = sex_cols) plot(c_tsne100[,1], c_tsne100[,2], main = &quot;Perplexity = 100&quot;, col = sex_cols) Run UMAP library(umap) c_umap &lt;- umap(crabs_meas) str(c_umap) ## List of 4 ## $ layout: num [1:200, 1:2] 0.0249 0.1114 0.246 0.4202 0.5572 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:200] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : NULL ## $ data : num [1:200, 1:5] 8.1 8.8 9.2 9.6 9.8 10.8 11.1 11.6 11.8 11.8 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:200] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:5] &quot;FL&quot; &quot;RW&quot; &quot;CL&quot; &quot;CW&quot; ... ## $ knn :List of 2 ## ..$ indexes : int [1:200, 1:15] 1 2 3 4 5 6 7 8 9 10 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:200] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : NULL ## ..$ distances: num [1:200, 1:15] 0 0 0 0 0 0 0 0 0 0 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:200] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : NULL ## ..- attr(*, &quot;class&quot;)= chr &quot;umap.knn&quot; ## $ config:List of 24 ## ..$ n_neighbors : int 15 ## ..$ n_components : int 2 ## ..$ metric : chr &quot;euclidean&quot; ## ..$ n_epochs : int 200 ## ..$ input : chr &quot;data&quot; ## ..$ init : chr &quot;spectral&quot; ## ..$ min_dist : num 0.1 ## ..$ set_op_mix_ratio : num 1 ## ..$ local_connectivity : num 1 ## ..$ bandwidth : num 1 ## ..$ alpha : num 1 ## ..$ gamma : num 1 ## ..$ negative_sample_rate: int 5 ## ..$ a : num 1.58 ## ..$ b : num 0.895 ## ..$ spread : num 1 ## ..$ random_state : int 249729221 ## ..$ transform_state : int NA ## ..$ knn : logi NA ## ..$ knn_repeats : num 1 ## ..$ verbose : logi FALSE ## ..$ umap_learn_args : logi NA ## ..$ method : chr &quot;naive&quot; ## ..$ metric.function :function (m, origin, targets) ## ..- attr(*, &quot;class&quot;)= chr &quot;umap.config&quot; ## - attr(*, &quot;class&quot;)= chr &quot;umap&quot; par(mfrow=c(1,2)) plot(c_umap$layout[,1], c_umap$layout[,2], col = species_cols, pch = 19, main = &quot;Colored by species&quot;) plot(c_umap$layout[,1], c_umap$layout[,2], col = sex_cols, pch = 19, main = &quot;Colored by sex&quot;) "],["module-3-generalized-linear-models.html", "Module 3: Generalized Linear Models Essential R: Read tables from files and merge Read data from files and explore variable distribution Explore missing data Create plots with ggplot2 to explore variable relationships Fit binary response variable using glm() and logistic regression Exercise", " Module 3: Generalized Linear Models In this module we’re going to cover: Reading data from files and evaluating missingness Creating publication-quality plots with ggplot2 Fitting models with binary outcomes using generalized linear models Essential R: Read tables from files and merge For this we’re going to use two data files available in the course data directory. Download these for use and put them in your R working directory. tomerge1.csv : comma-separated values tomerge2.txt : tab-delimited values Use read.delim() to read in tables. Note the use of the sep= parameter to indicate what the column separator is: x1 &lt;- read.delim(&quot;data/tomerge1.csv&quot;,sep=&quot;,&quot;) head(x1) ## Sample_ID Exposure Biomarker_value ## 1 A 0 35 ## 2 B 0 22 ## 3 C 0 91 ## 4 D 0 3 ## 5 E 1 56 ## 6 F 1 37 x2 &lt;- read.delim(&quot;data/tomerge2.txt&quot;,sep=&quot;\\t&quot;) head(x2) ## sampleID.Exposure_level.Biomarker2_detected ## 1 A 0.654055170016363 1 ## 2 B 0.672028516884893 0 ## 3 C 0.886463718488812 1 ## 4 D 0.284332562703639 0 ## 5 E 0.041668388992548 0 ## 6 F 0.452635339228436 1 Use merge() to combine the two tables by sample ID. Note the use of by.x and by.y to tell merge() which columns are equivalent: Read data from files and explore variable distribution We’re going to use a popular dataset for data analysis, pertaining to survival of passengers aboard the Titanic. Download the dataset here and copy it into your working directory. Let’s read in the data from a file: dat &lt;- read.delim( &quot;data/titanic.csv&quot;, sep=&quot;,&quot; # indicate the column separator ) Examine the columns: head(dat) ## PassengerId Survived Pclass ## 1 1 0 3 ## 2 2 1 1 ## 3 3 1 3 ## 4 4 1 1 ## 5 5 0 3 ## 6 6 0 3 ## Name Sex Age SibSp ## 1 Braund, Mr. Owen Harris male 22 1 ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 ## 3 Heikkinen, Miss. Laina female 26 0 ## 4 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 ## 5 Allen, Mr. William Henry male 35 0 ## 6 Moran, Mr. James male NA 0 ## Parch Ticket Fare Cabin Embarked ## 1 0 A/5 21171 7.2500 S ## 2 0 PC 17599 71.2833 C85 C ## 3 0 STON/O2. 3101282 7.9250 S ## 4 0 113803 53.1000 C123 S ## 5 0 373450 8.0500 S ## 6 0 330877 8.4583 Q Some of the columns are categorical, use table() to look at the tallies: Examine the columns: table(dat$Survived) ## ## 0 1 ## 549 342 table(dat$Pclass) ## ## 1 2 3 ## 216 184 491 Use summary() to look at continuous-valued data: summary(dat$Age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.42 20.12 28.00 29.70 38.00 80.00 177 Notice that there are 177 NA (missing) values for age. Let’s visualize the missing data more systematically. Explore missing data For this let’s use a little script that converts a table into black and white squares to visualize missing data. For this, install the plotrix package if (!requireNamespace(&quot;plotrix&quot;, quietly = TRUE)) install.packages(&quot;plotrix&quot;) suppressMessages(require(plotrix)) #&#39; show data missingness as a chequered matrix #&#39; #&#39; @param x (matrix) data matrix. #&#39; @param outFile (char) path to file for printing graph #&#39; @param wd (numeric) width in inches #&#39; @param ht (numeric) height in inches #&#39; @return plots missingness matrix to file #&#39; @import plotrix #&#39; @export plotMissMat &lt;- function(x,xlab=&quot;columns&quot;, ylab=&quot;rows&quot;,border=NA) { x &lt;- !is.na(x) class(x) &lt;- &quot;numeric&quot; color2D.matplot(x,show.values=FALSE,axes=FALSE, cs1=c(1,0),cs2=c(1,0),cs3=c(1,0),border=border, cex=0.8, xlab=xlab,ylab=ylab) } Let’s look at the missingness in the Titanic dataset. Missing data is shown as a white cell, and non-missing data is shown in black. plotMissMat(dat) We can see a column with many missing values. This is probably the “age” data. Let’s count the number of missing values on a per-column level. For this we combine is.na(), which returns a TRUE/FALSE value for NA values, and colSums() which adds up the TRUE values down the columns. colSums(is.na(dat)) ## PassengerId Survived Pclass Name Sex ## 0 0 0 0 0 ## Age SibSp Parch Ticket Fare ## 177 0 0 0 0 ## Cabin Embarked ## 0 0 This confirms that Age is the only column with missing data. Now let’s explore the data using plots. Create plots with ggplot2 to explore variable relationships ggplot2 is a popular plotting package that uses an additive paradigm to build plots. The ggplot2 website is a wealth of reference information, so here we just touch on the basics to get you started. Anytime you need to generate a specific kind of plot, the website will most likely have documentation for how to achieve it. Let’s start by creating a scatterplot. For this let’s load a dataset measuring statistics around quality of life in US states in the late 70’s: state.x77 &lt;- as.data.frame(state.x77) head(state.x77) ## Population Income Illiteracy Life Exp Murder HS Grad Frost ## Alabama 3615 3624 2.1 69.05 15.1 41.3 20 ## Alaska 365 6315 1.5 69.31 11.3 66.7 152 ## Arizona 2212 4530 1.8 70.55 7.8 58.1 15 ## Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 ## California 21198 5114 1.1 71.71 10.3 62.6 20 ## Colorado 2541 4884 0.7 72.06 6.8 63.9 166 ## Area ## Alabama 50708 ## Alaska 566432 ## Arizona 113417 ## Arkansas 51945 ## California 156361 ## Colorado 103766 Create a base plot using the ggplot function. Then “add” a scatterplot to it. Notice that the plot has been assigned to a variable named p. This setup is standard for ggplot2 and allows multiple visualizations to be applied to the same base plot. We use aes to tell ggplot what the x and y axes are, and later, if we want to colour-code by a particular column. require(ggplot2) p &lt;- ggplot(state.x77, aes(x = Illiteracy,y = Income) ) p &lt;- p + geom_point() # scatter plot p Now let’s add confidence intervals: p + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; It looks like there is a negative relationship between illiteracy and income. We can confirm this by looking at correlation: x &lt;- state.x77$Illiteracy y &lt;- state.x77$Income cor.test(x,y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = -3.3668, df = 48, p-value = 0.001505 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.6378257 -0.1807128 ## sample estimates: ## cor ## -0.4370752 cor.test(x,y)$p.value ## [1] 0.001505073 Let’s now examine categorical variables. In the titanic set, let’s look at the fare paid based on passenger class: p &lt;- ggplot(dat) p + geom_boxplot( aes(x = factor(Pclass), # make categorical y = Fare)) We can use barplots to examine counts and proportions. Let’s look at number of survivors, split by passenger class p + geom_bar( aes(fill=factor(Survived), Pclass) ) The plot above shows count data. Let’s convert this to proportions. We can see that the fraction of non-survivors in “Class 3” is high. p + geom_bar( aes(fill=factor(Survived), Pclass), position = &quot;fill&quot; ) How about males versus females? p + geom_bar( aes(fill=factor(Survived), Sex), position = &quot;fill&quot; ) Fit binary response variable using glm() and logistic regression Let’s fit a model to a binary outcome. For this we load a dataset that measures physiological variables in a cohort of Pima Indians. require(mlbench) data(PimaIndiansDiabetes2) # type ?PimaIndiansDiabetes2 to learn more about the dataset. dat &lt;- PimaIndiansDiabetes2 head(dat) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 NA 33.6 0.627 50 pos ## 2 1 85 66 29 NA 26.6 0.351 31 neg ## 3 8 183 64 NA NA 23.3 0.672 32 pos ## 4 1 89 66 23 94 28.1 0.167 21 neg ## 5 0 137 40 35 168 43.1 2.288 33 pos ## 6 5 116 74 NA NA 25.6 0.201 30 neg Let’s look at the impact of blood glucose levels on diabetes diagnosis. First let’s make a scatterplot. Could there be a relationship? p &lt;- ggplot(dat, aes(x = glucose, y = factor(diabetes))) p + geom_point() ## Warning: Removed 5 rows containing missing values (`geom_point()`). Could this be fit with a linear model? p &lt;- ggplot(dat, aes(x = glucose, y = factor(diabetes))) p + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; ## Warning: Removed 5 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 5 rows containing missing values (`geom_point()`). This is a situation where a logistic regression model would be an appropriate choice. We’re going to use glm() to fit a model to these data: mod &lt;- glm(factor(diabetes)~glucose, dat, family = &quot;binomial&quot; # set to model binary outcome ) summary(mod) ## ## Call: ## glm(formula = factor(diabetes) ~ glucose, family = &quot;binomial&quot;, ## data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1846 -0.7773 -0.5094 0.8232 2.2896 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.715088 0.438100 -13.04 &lt;2e-16 *** ## glucose 0.040634 0.003382 12.01 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 986.70 on 762 degrees of freedom ## Residual deviance: 786.56 on 761 degrees of freedom ## (5 observations deleted due to missingness) ## AIC: 790.56 ## ## Number of Fisher Scoring iterations: 4 Which factors explain a diabetes diagnosis? What if we include a couple other factors? mod &lt;- glm(factor(diabetes)~ glucose + pregnant + age + pressure + triceps, dat, family = &quot;binomial&quot;) summary(mod) ## ## Call: ## glm(formula = factor(diabetes) ~ glucose + pregnant + age + pressure + ## triceps, family = &quot;binomial&quot;, data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1790 -0.6813 -0.4061 0.7167 2.3204 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.6685706 0.8451262 -9.074 &lt; 2e-16 *** ## glucose 0.0365926 0.0041105 8.902 &lt; 2e-16 *** ## pregnant 0.0972493 0.0417560 2.329 0.019860 * ## age 0.0249707 0.0135945 1.837 0.066235 . ## pressure -0.0009321 0.0099322 -0.094 0.925230 ## triceps 0.0420045 0.0117044 3.589 0.000332 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 678.40 on 533 degrees of freedom ## Residual deviance: 494.38 on 528 degrees of freedom ## (234 observations deleted due to missingness) ## AIC: 506.38 ## ## Number of Fisher Scoring iterations: 5 Note: This morning’s session only intends to introduce you to fitting non-linear models. In practice you may need to do more work to test multiple models to ascertain best fits your data, using measures such as goodness-of-fit. You will also likely compute odds ratio (odds of increased Y per unit increase X), which is out of scope for the current tutorial. We strongly recommend that you learn these topics before applying these methods to your own data. Exercise Now let’s apply the ideas above to a dataset for classifying a breast cell as being either benign or malignant. data(BreastCancer) bc &lt;- BreastCancer for (k in 2:10) # altered for current lab bc[,k] &lt;- as.numeric(bc[,k]) head(bc) ## Id Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size ## 1 1000025 5 1 1 1 2 ## 2 1002945 5 4 4 5 7 ## 3 1015425 3 1 1 1 2 ## 4 1016277 6 8 8 1 3 ## 5 1017023 4 1 1 3 2 ## 6 1017122 8 10 10 8 7 ## Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses Class ## 1 1 3 1 1 benign ## 2 10 3 2 1 benign ## 3 2 3 1 1 benign ## 4 4 3 7 1 benign ## 5 1 3 1 1 benign ## 6 10 9 7 1 malignant Learn more about the dataset: ?BreastCancer For your exercise, answer the following questions: Is there missing data? Which columns are Use plots to explore the relationship between explanatory variables. Fit a logistic model to identify which factors explain cell size. "],["module-4-finding-differentially-expressed-genes-with-rna-seq.html", "Module 4: Finding differentially expressed genes with RNA-seq Mini introduction to BioConductor Fetch breast cancer data using curatedTCGAData Differential expression analysis with edgeR Exercise", " Module 4: Finding differentially expressed genes with RNA-seq We are going to look at genes differentially expressed between Luminal and Basal subtypes of breast cancer. For this we are going to download data from The Cancer Genome Atlas [ADD LINK] readily available in BioConductor. Mini introduction to BioConductor BioConductor is the most popular software repository for genomics data analysis, and is worth exploring in detail. This R-based ecosystem is open-source and community built. It contains software LINK packages but also DATA and ANNOTATION packages. Examples of packages in BioConductor: Data package: The entire human genome sequence e.g., for the current GRCH38 build Data package: Multi-modal genomic data from The Cancer Genome Atlas for various tumour types Annotation package: Annotation for the Illumina DNA methylation array Software: DEseq to process RNAseq data, minfi to process DNA methylation array data, CHIPseeker for Chip-Seq data, etc., As we will see, BioConductor also has specialized data structures for bioinformatics operations. Data structures are specialized, standardized formats for organizing, accessing, modifying and storing data. Examples of BioConductor data structures: GenomicRanges: for working with genomic coordinates SummarizedExperiment: container for storing data and metadata about an experiment MultiAssayExperiment: container for experiments where multiple genomic assays were run on the same samples BioConductor uses its own package manager to install packages. Instead of install.packages() we use, BiocManager::install(). if (!requireNamespace(&quot;curatedTCGAData&quot;, quietly = TRUE)) BiocManager::install(&quot;curatedTCGAData&quot;) Fetch breast cancer data using curatedTCGAData Load the package: suppressMessages(library(curatedTCGAData)) Let’s take a look at the available data for breast cancer, without downloading anything (set dry.run=TRUE). Each row shows a data layer available. e.g., “mRNA” is transcriptomic data, “CNA” is chromosomal copy number aberration data, etc., curatedTCGAData(diseaseCode=&quot;BRCA&quot;, assays=&quot;*&quot;,dry.run=TRUE, version=&quot;2.0.1&quot;) ## snapshotDate(): 2021-10-19 ## See &#39;?curatedTCGAData&#39; for &#39;diseaseCode&#39; and &#39;assays&#39; inputs ## ah_id title file_size ## 1 EH4769 BRCA_CNASeq-20160128 0 Mb ## 2 EH4770 BRCA_CNASNP-20160128 9.8 Mb ## 3 EH4771 BRCA_CNVSNP-20160128 2.8 Mb ## 4 EH4773 BRCA_GISTIC_AllByGene-20160128 1.2 Mb ## 5 EH4774 BRCA_GISTIC_Peaks-20160128 0 Mb ## 6 EH4775 BRCA_GISTIC_ThresholdedByGene-20160128 0.3 Mb ## 7 EH4777 BRCA_Methylation_methyl27-20160128_assays 60.7 Mb ## 8 EH4778 BRCA_Methylation_methyl27-20160128_se 0.4 Mb ## 9 EH4779 BRCA_Methylation_methyl450-20160128_assays 2646.4 Mb ## 10 EH4780 BRCA_Methylation_methyl450-20160128_se 6 Mb ## 11 EH4781 BRCA_miRNASeqGene-20160128 0.6 Mb ## 12 EH4782 BRCA_mRNAArray-20160128 27.3 Mb ## 13 EH4783 BRCA_Mutation-20160128 4.5 Mb ## 14 EH4784 BRCA_RNASeq2Gene-20160128 43.1 Mb ## 15 EH4785 BRCA_RNASeq2GeneNorm-20160128 64.5 Mb ## 16 EH4786 BRCA_RNASeqGene-20160128 30 Mb ## 17 EH4787 BRCA_RPPAArray-20160128 1.5 Mb ## rdataclass rdatadateadded rdatadateremoved ## 1 RaggedExperiment 2021-01-27 &lt;NA&gt; ## 2 RaggedExperiment 2021-01-27 &lt;NA&gt; ## 3 RaggedExperiment 2021-01-27 &lt;NA&gt; ## 4 SummarizedExperiment 2021-01-27 &lt;NA&gt; ## 5 RangedSummarizedExperiment 2021-01-27 &lt;NA&gt; ## 6 SummarizedExperiment 2021-01-27 &lt;NA&gt; ## 7 SummarizedExperiment 2021-01-27 &lt;NA&gt; ## 8 SummarizedExperiment 2021-01-27 &lt;NA&gt; ## 9 RaggedExperiment 2021-01-27 &lt;NA&gt; ## 10 SummarizedExperiment 2021-01-27 &lt;NA&gt; ## 11 SummarizedExperiment 2021-01-27 &lt;NA&gt; ## 12 SummarizedExperiment 2021-01-27 &lt;NA&gt; ## 13 SummarizedExperiment 2021-01-27 &lt;NA&gt; ## 14 DFrame 2021-01-27 &lt;NA&gt; ## 15 SummarizedExperiment 2021-01-27 &lt;NA&gt; ## 16 SummarizedExperiment 2021-01-27 &lt;NA&gt; ## 17 SummarizedExperiment 2021-01-27 &lt;NA&gt; We want to get unprocessed gene expression read counts, so let’s fetch the BRCA_RNASeq2Gene-20160128 layer. You can use ?curatedTCGAData to see a description of all data layers. brca &lt;- suppressMessages( curatedTCGAData( &quot;BRCA&quot;, assays=&quot;RNASeqGene&quot;, dry.run=FALSE, version=&quot;2.0.1&quot;) ) This call returns a MultiAssayExperiment object. Recall that this is a container for storing multiple assays performed on the same set of samples. See this tutorial to learn more. Let’s briefly explore the brca MultiAssayExperiment object. brca ## A MultiAssayExperiment object of 1 listed ## experiment with a user-defined name and respective class. ## Containing an ExperimentList class object of length 1: ## [1] BRCA_RNASeqGene-20160128: SummarizedExperiment with 20502 rows and 878 columns ## Functionality: ## experiments() - obtain the ExperimentList instance ## colData() - the primary/phenotype DataFrame ## sampleMap() - the sample coordination DataFrame ## `$`, `[`, `[[` - extract colData columns, subset, or experiment ## *Format() - convert into a long or wide DataFrame ## assays() - convert ExperimentList to a SimpleList of matrices ## exportClass() - save data to flat files assays() returns a list with all -omic data associated with this object. Here we just have the one we downloaded. summary(assays(brca)) ## [1] &quot;List object of length 1 with 0 metadata columns&quot; names() shows the datatypes in each slot of assays(): names(assays(brca)) ## [1] &quot;BRCA_RNASeqGene-20160128&quot; So gene expression in slot 1. We can subset the data to see what it looks like. Let’s look at just the first five measures, for the first 10 samples: xpr &lt;- assays(brca)[[1]] head(xpr[1:10,1:5]) ## TCGA-A1-A0SB-01A-11R-A144-07 TCGA-A1-A0SD-01A-11R-A115-07 ## A1BG 164 546 ## A1CF 0 0 ## A2BP1 22 1 ## A2LD1 127 331 ## A2ML1 94 144 ## A2M 102123 107181 ## TCGA-A1-A0SE-01A-11R-A084-07 TCGA-A1-A0SF-01A-11R-A144-07 ## A1BG 1341 836 ## A1CF 0 1 ## A2BP1 2 0 ## A2LD1 498 526 ## A2ML1 114 77 ## A2M 101192 50316 ## TCGA-A1-A0SG-01A-11R-A144-07 ## A1BG 512 ## A1CF 3 ## A2BP1 25 ## A2LD1 451 ## A2ML1 76 ## A2M 45826 How many measures do we have? nrow(xpr) ## [1] 20502 Prepare data for differential expression analysis Process the expression values: cnames &lt;- colnames(xpr) hpos &lt;- gregexpr(&quot;-&quot;,cnames) tmp &lt;- sapply(1:length(cnames),function(i) {substr(cnames[i],1,hpos[[i]][3]-1)}) idx &lt;- !duplicated(tmp) # subset values tmp &lt;- tmp[idx] xpr &lt;- xpr[,idx] colnames(xpr) &lt;- tmp Patient metadata is contained in the colData() slot. Rows contain data for each patient and columns contain measures such as clinical characteristics: pheno &lt;- colData(brca) colnames(pheno)[1:20] ## [1] &quot;patientID&quot; ## [2] &quot;years_to_birth&quot; ## [3] &quot;vital_status&quot; ## [4] &quot;days_to_death&quot; ## [5] &quot;days_to_last_followup&quot; ## [6] &quot;tumor_tissue_site&quot; ## [7] &quot;pathologic_stage&quot; ## [8] &quot;pathology_T_stage&quot; ## [9] &quot;pathology_N_stage&quot; ## [10] &quot;pathology_M_stage&quot; ## [11] &quot;gender&quot; ## [12] &quot;date_of_initial_pathologic_diagnosis&quot; ## [13] &quot;days_to_last_known_alive&quot; ## [14] &quot;radiation_therapy&quot; ## [15] &quot;histological_type&quot; ## [16] &quot;number_of_lymph_nodes&quot; ## [17] &quot;race&quot; ## [18] &quot;ethnicity&quot; ## [19] &quot;admin.bcr&quot; ## [20] &quot;admin.day_of_dcc_upload&quot; head(pheno[,1:5]) ## DataFrame with 6 rows and 5 columns ## patientID years_to_birth vital_status days_to_death ## &lt;character&gt; &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; ## TCGA-A1-A0SB TCGA-A1-A0SB 70 0 NA ## TCGA-A1-A0SD TCGA-A1-A0SD 59 0 NA ## TCGA-A1-A0SE TCGA-A1-A0SE 56 0 NA ## TCGA-A1-A0SF TCGA-A1-A0SF 54 0 NA ## TCGA-A1-A0SG TCGA-A1-A0SG 61 0 NA ## TCGA-A1-A0SH TCGA-A1-A0SH 39 0 NA ## days_to_last_followup ## &lt;integer&gt; ## TCGA-A1-A0SB 259 ## TCGA-A1-A0SD 437 ## TCGA-A1-A0SE 1321 ## TCGA-A1-A0SF 1463 ## TCGA-A1-A0SG 434 ## TCGA-A1-A0SH 1437 Let’s confirm that samples are in the same order in the xpr matrix and in the metadata table pheno. ids_in_both &lt;- intersect(pheno$patientID, colnames(xpr)) pheno &lt;- pheno[which(pheno$patientID %in% ids_in_both),] xpr &lt;- xpr[,which(colnames(xpr) %in% ids_in_both)] if (all.equal(pheno$patientID, colnames(xpr))!=TRUE) { midx &lt;- match(pheno$patientID, colnames(xpr)) xpr &lt;- xpr[,midx] } Always check that samples are in the same order in the tables you are comparing, and use match() to reorder them if necessary. If they are not in the same order, you are matching data to the wrong sample, and your results will be wrong. This table has many columns. Let’s just keep those related to tumour type according to the PAM50 classification. What tumour types do we have in this dataset? pheno &lt;- pheno[,c(&quot;patientID&quot;,&quot;PAM50.mRNA&quot;)] table(pheno$PAM50.mRNA) ## ## Basal-like HER2-enriched Luminal A Luminal B Normal-like ## 90 55 216 117 7 Let’s limit the comparison to Luminal A and Basal type tumours. idx &lt;- which(pheno$PAM50.mRNA %in% c(&quot;Luminal A&quot;,&quot;Basal-like&quot;)) pheno &lt;- pheno[idx,] xpr &lt;- xpr[,idx] dim(pheno) ## [1] 306 2 dim(xpr) ## [1] 20502 306 Differential expression analysis with edgeR Now that the data are prepared, let’s created a DGEList object (DGE stands for “Differential Gene Expression”). This object is what we will use for our differential expression analysis. Note: Make phenotype of interest categorical. In R that means converting to a factor type with categorical levels. You can think of levels as ordinal representations (e.g., first level = 1, second = 2, etc., ) If levels= are not set, the default uses alphabetical order. We recommend explicitly setting levels so that there are no assumptions. Load the edgeR package: suppressMessages(require(edgeR)) Let’s create a DGEList object for the differential expression analysis. Note that group must be a categorical variable (use factor() to convert it to one): group &lt;- factor( pheno$PAM50.mRNA, levels=c(&quot;Luminal A&quot;,&quot;Basal-like&quot;) ) dge &lt;- DGEList( counts = xpr, group = group ) Remove low-count genes: To filter low count genes, we’re going to use a normalized count measure called cpm (counts per million). We are going to keep genes with 100 or greater counts per million for at least two samples: (dge$counts[1:6,1:20]) ## TCGA-A1-A0SD TCGA-A1-A0SE TCGA-A1-A0SH TCGA-A1-A0SJ TCGA-A1-A0SK ## A1BG 546 1341 1126 1433 626 ## A1CF 0 0 1 0 1 ## A2BP1 1 2 4 1 1 ## A2LD1 331 498 838 831 2948 ## A2ML1 144 114 127 119 367 ## A2M 107181 101192 81857 128591 27972 ## TCGA-A1-A0SO TCGA-A2-A04N TCGA-A2-A04P TCGA-A2-A04Q TCGA-A2-A04T ## A1BG 2502 592 503 660 485 ## A1CF 4 1 4 1 3 ## A2BP1 0 0 2 1 11 ## A2LD1 883 487 384 694 534 ## A2ML1 335 87 7435 13543 89424 ## A2M 32286 35748 38122 70012 76814 ## TCGA-A2-A04U TCGA-A2-A04V TCGA-A2-A04Y TCGA-A2-A0CM TCGA-A2-A0CP ## A1BG 1269 870 1392 297 1240 ## A1CF 5 2 1 0 0 ## A2BP1 0 7 18 0 2 ## A2LD1 200 454 932 161 335 ## A2ML1 3088 160 250 4770 76 ## A2M 178113 50336 75809 37519 76407 ## TCGA-A2-A0CQ TCGA-A2-A0CS TCGA-A2-A0CU TCGA-A2-A0CV TCGA-A2-A0CZ ## A1BG 919 1422 688 532 1628 ## A1CF 1 0 0 0 4 ## A2BP1 1 8 1 6 3 ## A2LD1 111 589 329 458 468 ## A2ML1 217 65 168 162 102 ## A2M 45968 18353 197374 85242 85978 Look at counts per million using cpm: cpm(dge)[1:5,1:5] ## TCGA-A1-A0SD TCGA-A1-A0SE TCGA-A1-A0SH TCGA-A1-A0SJ TCGA-A1-A0SK ## A1BG 6.40538988 10.40014782 7.874879329 11.979028558 4.544931080 ## A1CF 0.00000000 0.00000000 0.006993676 0.000000000 0.007260273 ## A2BP1 0.01173148 0.01551103 0.027974705 0.008359406 0.007260273 ## A2LD1 3.88312097 3.86224729 5.860700602 6.946666247 21.403285660 ## A2ML1 1.68933359 0.88412890 0.888196869 0.994769294 2.664520298 This next line is a bit complex so let’s unpack it: We are using cpm(dge)&gt;100 as a logical test (“which genes have cpm &gt; 100?”). For each gene, we want that test to be true for at least two samples. For this we use rowSums() to add up how many samples meet that criteria. dim(dge) #before ## [1] 20502 306 tokeep &lt;- rowSums(cpm(dge)&gt;100) &gt;= 2 dge &lt;- dge[tokeep,,keep.lib.sizes = FALSE] dim(dge) #after ## [1] 7699 306 Normalize the data: dge &lt;- calcNormFactors(dge) Visualize the data: plotMDS( dge, col=as.numeric(dge$samples$group), pch=16 ) legend( &quot;bottomleft&quot;, as.character(unique(dge$samples$group)), col=c(1,2), pch=16 ) Let’s create a model design to identify genes with a group effect: group &lt;- dge$samples$group mod &lt;- model.matrix(~group) Estimate variation (“dispersion”) for each gene: dge &lt;- estimateDisp(dge, mod) Call differentially expressed genes. Here we: fit a model for each gene, using glmFit we have built in an estimate of gene-wise dispersion to better identify treatment effect (or “contrast”) for each gene, we run a likelihood ratio test which compares which model fits the data better: a null model (treatment effect = 0) or a full model (treatment effect is non-zero) Note that coef=2 fetches the effects for the treatment effect; coef=1 would fetch effects of the intercept term.   fit &lt;- glmFit(dge,mod) diffEx &lt;- glmLRT(fit, coef = 2) # get coefficients for group term Look at the top 10 differentially expressed genes: tt &lt;- topTags(diffEx, n=10) tt ## Coefficient: groupBasal-like ## logFC logCPM LR PValue FDR ## ART3 4.437401 3.455108 795.7399 4.552774e-175 3.505180e-171 ## EN1 4.493149 4.469186 568.8034 1.022382e-125 3.935658e-122 ## MTHFD1L 2.072720 4.744988 548.6874 2.429549e-121 6.235032e-118 ## CLDN6 5.917771 1.735389 528.0710 7.423651e-117 1.428867e-113 ## CENPW 2.628365 3.806891 523.0933 8.985654e-116 1.383611e-112 ## IL12RB2 3.981078 3.080989 507.2309 2.539169e-112 3.258177e-109 ## TTLL4 1.857331 5.441088 504.0036 1.279029e-111 1.406749e-108 ## CXorf49B 6.007052 1.678586 484.6339 2.096210e-107 2.017340e-104 ## GFRA3 4.890609 1.725042 468.7621 5.958600e-104 5.097251e-101 ## HAPLN3 2.737617 5.215775 460.3033 4.129466e-102 3.179276e-99 For the next steps we’re going to need stats on all the genes we’ve tested. So let’s get those: tt &lt;- as.data.frame( topTags(diffEx, n=nrow(dge) ) ) A QQplot directly compares the pvalues from our statistical tests to the expected values from a random uniform distribution (p-value selected at random). A deviation from the x=y line (diagonal) towards the top indicates an enrichment of signal. qqplot( tt$PValue, runif(nrow(tt)), # randomly sample from uniform distribution xlab=&quot;p-values from real data&quot;, ylab=&quot;Randomly-sampled values from Uniform distribution&quot;, pch=16,cex=0.5 ) # x=y line as reference abline(0,1,col=&quot;red&quot;) Now let’s call differentially expressed genes using the decideTestDGE() function and use summary() to see how many genes are upregulated (value +1), downregulated (value -1) and not called as changed (value 0) diffEx2 &lt;- decideTestsDGE(diffEx, adjust.method=&quot;BH&quot;, p.value=0.05 ) summary(diffEx2) ## groupBasal-like ## Down 2813 ## NotSig 1681 ## Up 3205 A volcano plot can help visualize effect magnitude - log2 fold-change or log2FC in the table ` against the corresponding p-value. Here we create a volcano plot, and colour-code upregulated genes in red, and downregulated genes in blue. Note that we are combining two different tables, tt and diffEx2 so we need to ensure the order is the same. Otherwise the colours will be in the wrong order (try it for yourself!). midx &lt;- match(rownames(tt), rownames(diffEx2)) diffEx2 &lt;- diffEx2[midx,] cols &lt;- rep(&quot;black&quot;,nrow(diffEx2)) cols[which(diffEx2&gt;0 )] &lt;- &quot;red&quot; cols[which(diffEx2&lt;0)] &lt;- &quot;blue&quot; # volcano plot plot(tt$logFC,-log10(tt$PValue),pch=16, col=cols) abline(v=0,lty=3) Finally we can write our differential expression results out to file: write.table(tt,file=&quot;diffEx.results.txt&quot;, sep=&quot;\\t&quot;, col=TRUE, row=TRUE, quote=FALSE ) Exercise Install the yeastRNASeq package from Bioconductor and library it into your environment Import the geneLevelData using: data(yeastRNASeq) Learn about this data and then put it through the same workflow we just did for the breast cancer: Filter genes with &lt; 2 mean expression across all samples Create a new expression set object with your filtered genes Normalize and plot your data Create a model matrix for analysis Fit your model How many significantly up-regulated genes are there at the 5% FDR level? How many significantly down-regulated genes? How many in total Create a volcano plot Bonus: Create a histogram of p-values. Is there a signal? Is there anything about the data that might make you question the results? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
