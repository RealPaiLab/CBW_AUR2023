# Module 1: Exploratory Data Analysis and Clustering {-}

The goal of this lab is to examine correlation structure in a sample transcriptomic dataset using clustering.

Specifically, we will:

* Load and briefly explore transcriptomic measures from two mouse tissues
* Explore the correlation structure in the data using `pairs()`
* See the effect of running k-means and hierarchical clustering on the data
* Use `clValid` to find out what cluster number best separates groups

We will explore missing data in detail in Module 3.

## Load `mouse` data {-}

For this exercise we are going to load a dataset built into the `clValid` package.
This dataset measures 147 genes and expressed sequence tags in two developing mouse lineages: the neural crest cells and mesoderm-derived cells.
There are three samples per group.

Let's load the data.

```{r, class.source="codeblock",eval=TRUE}
suppressMessages(library(clValid))
data(mouse)
```

Use `str()` to see what types of data the columns have:

```{r, class.source="codeblock",eval=TRUE}
str(mouse)
```

Another command is `head()`:
```{r, class.source="codeblock",eval=TRUE}
head(mouse)
```

Summary provides useful information about the distribution of variables. Note that `FC` has categorical variables:

```{r, class.source="codeblock",eval=TRUE}
summary(mouse)
```

What are the values in `FC`?


```{r, class.source="codeblock",eval=TRUE}
table(mouse$FC)
```

Let's use `pairs()`  to look at pairwise scatterplots of the expression data in a single plot. We need to subset the columns with the expression data first:

```{r, class.source="codeblock",eval=TRUE}
mouse_exp = mouse[,c("M1","M2","M3","NC1","NC2","NC3")]
pairs(mouse_exp)
```

## `RColorBrewer` for colour palettes {-}

Visualization often requires assigning colours to categories of interest (e.g., two different tissue types, eight levels of doses).
`RColorBrewer` is a good package to pick a colour scheme for your project, as palettes are derived using colour science:
Let's look at all the palettes available.

```{r, class.source="codeblock",eval=TRUE}
library(RColorBrewer)
display.brewer.all()
```

We're going to use `RColorBrewer` to assign colours to our heatmap below.

## Correlations, distances, and clustering {-}

Let's look at sample correlation matrix. This should be a square matrix, equal to the number of samples.
We have six samples, so the correlation matrix should be `6x6`.



```{r, class.source="codeblock",eval=TRUE}
library(corrplot)
mouse_cor <- cor(mouse_exp)
dim(mouse_cor)
round(mouse_cor,2)
corrplot(mouse_cor, method="color")
```

* Which samples appear to be best correlated with each other?
* Which samples don't appear to be as well correlated with each other?

## Hierarchical clustering {-}

Hierarchical clustering requires distances between samples. Let's use `dist()` to compute these distances, and `hclust()` to generate the hierarchical clustering object. 

Different values for `method` can produce different results. In our experience `complete` or `ward.D2` produce stable results.

```{r, class.source="codeblock",eval=TRUE}
d <- dist(log(mouse_exp))
h <- hclust(d,method="ward.D2")
plot(h)
```

Can you guess how many clusters could best fit the data?

Now let's add a heatmap to this dendrogram, so we can see the values of genes in each cluster. For this we will use the `heatmap()` function, which requires assigned cluster labels to each sample, and a dendrogram-generating function. 

So let's create these.

First we get cluster assignments by "cutting" the dendrogram for two clusters (something we expect from our experimental design). We use `cutree()` for this.  Then assign colours based on cluster assignment.
```{r, class.source="codeblock",eval=TRUE}
h2 <- cutree(h, k = 2)
h2cols <- c("orangered","blue")[h2]
```

Now define a hierarchical clustering function:
```{r, class.source="codeblock",eval=TRUE}

hclust_fun <- function(x){
    f <- hclust(x, method = "ward.D2");
    return(f)
}
```



And finally supply the two to the `heatmap()` function:
```{r, class.source="codeblock",eval=TRUE}
heatmap(
    as.matrix(mouse_exp),
    col = brewer.pal("Blues",n=8),
    hclustfun = hclust_fun,
    RowSideColors = h2cols, # use colours from cutree call above
    ColSideColors = c(
        rep("darkgreen",3),
        rep("deeppink2",3)
    )
)
```

Note that the two colours are completely divided (i.e., there is no interspersed red and blue). This is a good sign that the clustering function used the identical clustering method, here, `ward.D2`, to cluster the samples.

Compare this result to what happens if we try the same function call without clustering:

```{r, class.source="codeblock",eval=TRUE}
heatmap(
    as.matrix(mouse_exp),
    col = brewer.pal("Blues",n=8),
    RowSideColors = h2cols, # use colours from cutree call above
    ColSideColors = c(
        rep("darkgreen",3),
        rep("deeppink2",3)
    )
)
```

## K-means clustering {-}

Let's try using k-means clustering, asking for three clusters:

```{r, class.source="codeblock",eval=TRUE}
kclust <- kmeans(
    log(mouse_exp), 
    centers = 3
)
kclust
```

## Using `clValid` to determine number of clusters {-}

Use the `clValid()` function to validate clusters using the:

* Dunn index,
* silhouette scores, and
* connectivity

```{r, class.source="codeblock",eval=TRUE}
validation_data <- clValid(
    mouse_exp,
    2:6, # num. clusters to evaluate
    clMethods = c("hier","kmeans"), # methods to eval.
    validation = "internal"
)
```

Let's look at the results:
```{r, class.source="codeblock",eval=TRUE}
summary(validation_data)
```

All measures of clustering consistently indicate that **two** clusters best fit the data.

Now let's cluster:

```{r, class.source="codeblock",eval=TRUE}
d <- dist(log(mouse_exp))
h <- hclust(d,method="ward.D2")
cluster_ids <- cutree(h, k = 2)
clust_colors <- c("dodgerblue","orangered")[cluster_ids]

heatmap(
    as.matrix(mouse_exp),
    col = brewer.pal("Blues",n=8),
    hclustfun = hclust_fun,
    RowSideColors = clust_colors, # kmeans labels
    ColSideColors = c(
        rep("darkgreen",3),
        rep("deeppink2",3)
    )
)
```


## Exercise {-}

For your exercise, try the following:

* Load the MASS package using: `library(MASS)`
* Import `crabs` dataset using: `data(crabs)`
* Learn about this dataset using: `?crabs`
* Extract the numeric columns describing the crab measurements (“FL”, “RW”, “CL”, “CW”, “BD”)
* Cluster the numeric columns using your method of choice
* Plot and color your data by clusters, by species (`sp`), and `sex`
* Do your clusters seem to separate these groups in the same way? 


