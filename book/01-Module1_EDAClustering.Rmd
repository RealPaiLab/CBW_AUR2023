# Module 1: Exploratory Data Analysis and Clustering

## Example data

```{r, class.source="codeblock",eval=TRUE}
suppressMessages(library(clValid))
data(mouse)
```

Learn more about the mouse data:
```{r, class.source="codeblock",eval=TRUE}
?mouse
```

And explore what's in the mouse data:

```{r, class.source="codeblock",eval=TRUE}
str(mouse)
```

Let's use `pairs()` to look at pairwise scatterplots. This is a useful way to explore data points. Let's first subset to three samples each from the mesenchymal and neural crest groups:

```{r, class.source="codeblock",eval=TRUE}
mouse_exp = mouse[,c("M1","M2","M3","NC1","NC2","NC3")]
pairs(mouse_exp)
```

Visualize mouse data using rgl
```{r, class.source="codeblock",eval=FALSE}
library(rgl)
open3d()
bg3d(color="white")
points3d(x=mouse_exp$M1, y=mouse_exp$NC2, z=mouse_exp$NC3, col="black")
```
Make points into spheres
```{r, class.source="codeblock",eval=FALSE}
rgl.spheres(
    x = mouse_exp$M1,
    y = mouse_exp$NC2,
    z = mouse_exp$NC3,
    radius=0.2,
    col="darkgreen"
)
```

Let's colour by functional group.
Use `RColorBrewer` to pick a colour palette. Assign colours to each category in `FC` (what is this?).
```{r, class.source="codeblock",eval=TRUE}
levels(mouse$FC)
library(RColorBrewer)

cols <- brewer.pal("Set1",n=9)[mouse$FC]
```
Use `table()` to examine the mapping between colours and levels of `mouse$FC`:
```{r, class.source="codeblock",eval=TRUE}
print(table(cols,mouse$FC))
```

Now plot the data points colour-coded:

```{r, class.source="codeblock",eval=FALSE}
rgl.spheres(
    x = mouse_exp$M1,
    y = mouse_exp$NC2,
    z = mouse_exp$NC3,
    radius=0.2,
    col=cols)
```

** Put in images for rgl **

### Exploring missingness

### The importance of bookkeeping

Introduce `browser()` for debugging. Introduce `message()`, keeping logs using `sink()` format.
Show example of a log file with a given date using `format(Sys.Date(),"%y%m%d"))`


### Correlations, distances, and clustering

Let's look at sample correlation:

```{r, class.source="codeblock",eval=TRUE}
library(corrplot)
mouse_cor <- cor(mouse_exp)
dim(mouse_cor)
round(mouse_cor,2)
corrplot(mouse_cor, method="color")
```

Let's calculate distances between samples.

Returns an upper triangle distance matrix. Stored as a vector that can be used for many clustering algorithms.
```{r, class.source="codeblock",eval=FALSE}
d <- dist(mouse_exp) 
```

Now in matrix format:
```{r, class.source="codeblock",eval=TRUE}
d <- as.matrix(dist(mouse_exp))
```

#### Hierarchical clustering

Let's create a hierarchical clustering diagram using the mouse samples:
```{r, class.source="codeblock",eval=TRUE}
d <- dist(log(mouse_exp))
h <- hclust(d,method="complete")
plot(h)
```

Now let's expand this idea to show the hierarchical clustering and a heatmap representation of the clustered data. 

```{r, class.source="codeblock",eval=TRUE}
d <- dist(log(mouse_exp))
h <- hclust(d,method="ward.D2")
```
Get cluster assignments by "cutting" the dendrogram for two clusters (something we expect from our experimental design).
Then assign colours based on cluster assignment
```{r, class.source="codeblock",eval=TRUE}
h2 <- cutree(h, k = 2)
h2cols <- c("orangered","blue")[h2]
```

Now define a hierarchical clustering function. This is needed as input for the `heatmap()` function which will draw the final clustering heatmap:
```{r, class.source="codeblock",eval=TRUE}

hclust_fun <- function(x){
    f <- hclust(x, method = "ward.D2");
    return(f)
}

heatmap(
    as.matrix(mouse_exp),
    col = brewer.pal("Blues",n=8),
    hclustfun = hclust_fun,
    RowSideColors = h2cols, # use colours from cutree call above
    ColSideColors = c(
        rep("darkgreen",3),
        rep("deeppink2",3)
    )
)
```

Note that the two colours are completely divided (i.e., there is no interspersed red and blue). This is a good sign that the clustering function used the identical clustering method, here, `ward.D2`, to cluster the samples.
Compare this result to what happens if we try the same function call without clustering:

```{r, class.source="codeblock",eval=TRUE}
heatmap(
    as.matrix(mouse_exp),
    col = brewer.pal("Blues",n=8),
    hclustfun = hclust_fun,
    RowSideColors = h2cols, # use colours from cutree call above
    ColSideColors = c(
        rep("darkgreen",3),
        rep("deeppink2",3)
    )
)
```

#### K-means clustering

```{r, class.source="codeblock",eval=TRUE}
kclust <- kmeans(
    log(mouse_exp), 
    centers = 2
)
```

### Gaussian mixtures clustering

This code finds two clusters using Euclidean distance

```{r, class.source="codeblock",eval=TRUE}
library(ClusterR) # needed for GMM function

gclust <- GMM(
    mouse_exp, 
    gaussian_comps = 2, 
    dist_mode = 'eucl_dist'
)

gclust$Log_likelihood
```

### Using `clValid` to determine number of clusters

Use the `clValid()` function to validate clusters using the:
* Dunn index
* silhouette scores, and
* connectivity

```{r, class.source="codeblock",eval=TRUE}
validation_data <- clValid(
    mouse_exp,
    2:6, # num. clusters to evaluate
    clMethods = c("hier","kmeans"), # methods to eval.
    validation = "internal"
)
```

Let's look at the results:
```{r, class.source="codeblock",eval=TRUE}
summary(validation_data)
```

All measures of clustering consistently indicate that two clusters best fit the data.

Now let's cluster:

```{r, class.source="codeblock",eval=TRUE}
d <- dist(log(mouse_exp))
h <- hclust(d,method="ward.D2")
cluster_ids <- cutree(h, k = 2)
clust_colors <- c("dodgerblue","orangered")[cluster_ids]
```

Let's now look at the pairs plot, using the new cluster assignment:
```{r, class.source="codeblock",eval=TRUE}
pairs(
    mouse_exp,
    col = clust_colors
)
```

And look at the same data in 3D:

```{r, class.source="codeblock",eval=FALSE}
rgl.spheres(
    x = mouse_exp$M1,
    y = mouse_exp$NC2,
    z = mouse_exp$NC3,
    radius = 0.2,
    col = clust_colors
)
```









## Exercise

LINK TO LECTURE SLIDES

