--- 
title: "Analysis Using R"
subtitle: "Canadian Bioinformatics Workshop"
author: "Instructors: Shraddha Pai, Chaitra Sarathy"
date: "last modified `r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib]
biblio-style: nature
csl: nature.csl
link-citations: yes
github-repo: rstudio/bookdown-demo
favicon: images/favicon.ico
description: "Principles of exploratory data analysis, RNAseq differential expression, and generalized linear models"
---

# Welcome {-}
Welcome to [Analysis Using R](https://bioinformatics.ca/workshops-all/2023-analysis-using-r/) 2023.

## Meet your Faculty {-}

### Shraddha Pai {.unlisted .unnumbered}
Investigator I, OICR<br>
Assistant Professor, University of Toronto

<img src="images/ShraddhaPai.jpeg" width=200 height=150 ALIGN="right"  hspace=25>
Dr. Pai integrates genomics and computational methods to advance precision medicine. Her previous work involves DNA methylome-based biomarker discovery in psychosis, and building machine learning algorithms for patient classification from multi-modal data. The <a href="https://pailab.oicr.on.ca">Pai Lab at the Ontario Institute for Cancer Research</a> focuses on biomarker discovery for detection, diagnosis and prognosis in brain cancers and other brain-related disorders. 

### Chaitra Sarathy {.unlisted .unnumbered}
Scientific Associate <br>
Princess Margaret Cancer Centre<br>
University Health Network

<img src="images/Chaitra_Sarathy.jpeg" ALIGN="right" hspace=25>
Dr. Sarathy is a computational biologist with experience in software development. During her PhD, she applied mathematical modelling, network analysis and multi-omics integration to study complex diseases. She has contributed to open-source toolboxes (openCOBRA) and developed softwares (EFMviz & ComMet) to analyse genome-scale metabolic models. She currently works in the Bader lab’s MODiL team (Multi Omics Data Integration and Analysis) and with groups at PMCC, where she develops pipelines to analyse various omics data types and discover new drug targets in cancer.


### Ian Cheong {.unlisted .unnumbered}
<img src="images/IanCheong.jpg" ALIGN="right" hspace=25 width=200 height=200>
MSc. candidate<br>
University of Toronto


Ian is a Master's level candidate in the Department of Medical Biophysics at the University of Toronto. His thesis work in the Pai lab involves analysis of single-cell transcriptomes to find the link between brain development and the development of childhood brain cancer. 
&nbsp;&nbsp;<br>
&nbsp;&nbsp;<br>

### Nia Hughes {.unlisted .unnumbered}

<img src="images/nia_hughes.jpeg" ALIGN="right" hspace=25>
Program Manager<br>
Bioinformatics.ca<br>
<a href="mailto:nia.hughes@oicr.on.ca">nia.hughes@oicr.on.ca</a>


Nia is the Program Manager for Bioinformatics.ca, where she coordinates the Canadian Bioinformatics Workshop Series. Prior to starting at the OICR, she completed her M.Sc in Bioinformatics from the University of Guelph in 2020 before working there as a bioinformatician studying epigenetic and transcriptomic patterns across maize varieties.

<p></p>

## Pre-workshop Materials and Laptop Setup Instructions {-}

### Laptop Setup Instructions {-}

A checklist to setup your laptop can be found here. 

Install these tools on your laptop before coming to the workshop:

1. R (4.0+)
<i>Note:</i> MacBook users with an Apple silicon chip (e.g., M1 or M2) should install the "arm64" version of R, while MacBook users with an Intel chip should install the regular (64-bit) version of R. You can check your laptop's hardware specifications by clicking the Apple icon (top left corner) > About This Mac and verifying whether the chip is Apple or Intel.
2. Rstudio
3.  Make sure you have a robust internet browser such as Firefox, Safari or Chrome (not Internet Explorer).
4.  Make sure you have a PDF viewer (e.g. Adobe Acrobat, Preview or similar) or that you can read PDF files in your Web browser.

### R packages {-}

```{r, eval=TRUE}
pkgList <- c("tidyverse", "clValid","rgl","RColorBrewer","corrplot","ClusterR",
  "Rtsne","umap","BiocManager","mlbench","plotrix")
for (cur in pkgList){
  message(sprintf("\tChecking for %s ...", cur))
  if (!requireNamespace(cur, quietly = TRUE)) install.packages(cur)
}
if (!requireNamespace("edgeR", quietly = TRUE)) BiocManager::install("edgeR")
if (!requireNamespace("curatedTCGAData", quietly = TRUE)) BiocManager::install("curatedTCGAData")
```


# Lecture slides {-}

**Links TBA** :

* Module 1: Exploratory Data Analysis and Clustering
* Module 2: Dimensionality reduction for visualization and analysis
* Module 3: Differential expression analysis using RNAseq
* Module 4: Multiple hypothesis testing using generalized linear models


<!--chapter:end:index.Rmd-->

# Module 1: Exploratory Data Analysis and Clustering {-}

The goal of this lab is to examine correlation structure in a sample transcriptomic dataset using clustering.

Specifically, we will:

* Load and briefly explore transcriptomic measures from two mouse tissues
* Explore the correlation structure in the data using `pairs()`
* See the effect of running k-means and hierarchical clustering on the data
* Use `clValid` to find out what cluster number best separates groups

We will explore missing data in detail in Module 3.

## Load `mouse` data {-}

For this exercise we are going to load a dataset built into the `clValid` package.
This dataset measures 147 genes and expressed sequence tags in two developing mouse lineages: the neural crest cells and mesoderm-derived cells.
There are three samples per group.

Let's load the data.

```{r, class.source="codeblock",eval=TRUE}
suppressMessages(library(clValid))
data(mouse)
```

Use `str()` to see what types of data the columns have:

```{r, class.source="codeblock",eval=TRUE}
str(mouse)
```

Another command is `head()`:
```{r, class.source="codeblock",eval=TRUE}
head(mouse)
```

Summary provides useful information about the distribution of variables. Note that `FC` has categorical variables:

```{r, class.source="codeblock",eval=TRUE}
summary(mouse)
```

What are the values in `FC`?


```{r, class.source="codeblock",eval=TRUE}
table(mouse$FC)
```

Let's use `pairs()`  to look at pairwise scatterplots of the expression data in a single plot. We need to subset the columns with the expression data first:

```{r, class.source="codeblock",eval=TRUE}
mouse_exp = mouse[,c("M1","M2","M3","NC1","NC2","NC3")]
pairs(mouse_exp)
```

## `RColorBrewer` for colour palettes {-}

Visualization often requires assigning colours to categories of interest (e.g., two different tissue types, eight levels of doses).
`RColorBrewer` is a good package to pick a colour scheme for your project, as palettes are derived using colour science:
Let's look at all the palettes available.

```{r, class.source="codeblock",eval=TRUE}
library(RColorBrewer)
display.brewer.all()
```

We're going to use `RColorBrewer` to assign colours to our heatmap below.

## Correlations, distances, and clustering {-}

Let's look at sample correlation matrix. This should be a square matrix, equal to the number of samples.
We have six samples, so the correlation matrix should be `6x6`.



```{r, class.source="codeblock",eval=TRUE}
library(corrplot)
mouse_cor <- cor(mouse_exp)
dim(mouse_cor)
round(mouse_cor,2)
corrplot(mouse_cor, method="color")
```
* Which samples appear to be best correlated with each other?
* Which samples don't appear to be as well correlated with each other?

## Hierarchical clustering {-}

Hierarchical clustering requires distances between samples. Let's use `dist()` to compute these distances, and `hclust()` to generate the hierarchical clustering object. 

Different values for `method` can produce different results. In our experience `complete` or `ward.D2` produce stable results.

```{r, class.source="codeblock",eval=TRUE}
d <- dist(log(mouse_exp))
h <- hclust(d,method="ward.D2")
plot(h)
```
Can you guess how many clusters could best fit the data?

Now let's add a heatmap to this dendrogram, so we can see the values of genes in each cluster. For this we will use the `heatmap()` function, which requires assigned cluster labels to each sample, and a dendrogram-generating function. 

So let's create these.

First we get cluster assignments by "cutting" the dendrogram for two clusters (something we expect from our experimental design). We use `cutree()` for this.  Then assign colours based on cluster assignment.
```{r, class.source="codeblock",eval=TRUE}
h2 <- cutree(h, k = 2)
h2cols <- c("orangered","blue")[h2]
```

Now define a hierarchical clustering function:
```{r, class.source="codeblock",eval=TRUE}

hclust_fun <- function(x){
    f <- hclust(x, method = "ward.D2");
    return(f)
}
```



And finally supply the two to the `heatmap()` function:
```{r, class.source="codeblock",eval=TRUE}
heatmap(
    as.matrix(mouse_exp),
    col = brewer.pal("Blues",n=8),
    hclustfun = hclust_fun,
    RowSideColors = h2cols, # use colours from cutree call above
    ColSideColors = c(
        rep("darkgreen",3),
        rep("deeppink2",3)
    )
)
```

Note that the two colours are completely divided (i.e., there is no interspersed red and blue). This is a good sign that the clustering function used the identical clustering method, here, `ward.D2`, to cluster the samples.

Compare this result to what happens if we try the same function call without clustering:

```{r, class.source="codeblock",eval=TRUE}
heatmap(
    as.matrix(mouse_exp),
    col = brewer.pal("Blues",n=8),
    RowSideColors = h2cols, # use colours from cutree call above
    ColSideColors = c(
        rep("darkgreen",3),
        rep("deeppink2",3)
    )
)
```

## K-means clustering {-}

Let's try using k-means clustering, asking for three clusters:

```{r, class.source="codeblock",eval=TRUE}
kclust <- kmeans(
    log(mouse_exp), 
    centers = 3
)
kclust
```

## Using `clValid` to determine number of clusters {-}

Use the `clValid()` function to validate clusters using the:
* Dunn index
* silhouette scores, and
* connectivity

```{r, class.source="codeblock",eval=TRUE}
validation_data <- clValid(
    mouse_exp,
    2:6, # num. clusters to evaluate
    clMethods = c("hier","kmeans"), # methods to eval.
    validation = "internal"
)
```

Let's look at the results:
```{r, class.source="codeblock",eval=TRUE}
summary(validation_data)
```

All measures of clustering consistently indicate that **two** clusters best fit the data.

Now let's cluster:

```{r, class.source="codeblock",eval=TRUE}
d <- dist(log(mouse_exp))
h <- hclust(d,method="ward.D2")
cluster_ids <- cutree(h, k = 2)
clust_colors <- c("dodgerblue","orangered")[cluster_ids]

heatmap(
    as.matrix(mouse_exp),
    col = brewer.pal("Blues",n=8),
    hclustfun = hclust_fun,
    RowSideColors = clust_colors, # kmeans labels
    ColSideColors = c(
        rep("darkgreen",3),
        rep("deeppink2",3)
    )
)
```


## Exercise {-}

For your exercise, try the following:

* Library the MASS package using: `library(MASS)`
* Import `crabs` dataset using: `data(crabs)`
* Learn about this dataset using: `?crabs`
* Extract the numeric columns describing the crab measurements (“FL”, “RW”, “CL”, “CW”, “BD”)
* Cluster the numeric columns using your method of choice
* Plot and color your data by clusters, by species (`sp`), and `sex`
* Do your clusters seem to separate these groups in the same way? 



<!--chapter:end:01-Module1_EDAClustering.Rmd-->

# Module 1 Exercise Results {-}

Load packages and data, subset needed columns:
```{r, class.source="codeblock",eval=TRUE}
library(MASS)
data(crabs)
```

Learn more about the data:
```{r, class.source="codeblock",eval=TRUE}
?crabs
head(crabs)
```

Subset needed columns:
```{r, class.source="codeblock",eval=TRUE}
crabs_meas <-  crabs[,c("FL","RW","CL","CW","BD")]
```

Perform hierarchical clustering:

```{r, class.source="codeblock",eval=TRUE}
c_dist <- dist(crabs_meas)
c_hclust <- hclust(c_dist)
plot(c_hclust)
```

Colour-code samples based on cluster assignment. Assume there are two clusters.

```{r, class.source="codeblock",eval=TRUE}
c_clusters = cutree(c_hclust,k = 2)
```

Now create a pairs plot, but colour-code by: 
1. by gene-expression based clusters
2. by species
3. by sex

```{r, class.source="codeblock",eval=TRUE}
pairs(
    crabs_meas, 
    col = c("orchid","forestgreen")[c_clusters]
)

pairs(
    crabs_meas, 
    col = c("orchid","forestgreen")[factor(crabs$sp)]
)

pairs(
    crabs_meas, 
    col = c("orchid","forestgreen")[factor(crabs$sex)]
)
```


<!--chapter:end:02-Module1_Exercise.Rmd-->

# Module 2: Dimensionality reduction {-}

The goal of this lab is to learn how to reduce the dimension of your dataset.

We will learn three different methods commonly used for dimension reduction:
1. Principal Component Analysis
2. UMAP
3. tSNE


METHOD 1: PRINCIPAL COMPONENT ANALYSIS

Let's start with PCA. PCA is commonly used as one step in a series of analyses. The goal of PCA is to explain most of the variability in the data with a smaller number of variables than the original data set. You can use PCA to explain the variability in your data using fewer variables. Typically, it is useful to identify outliers and determine if there's batch effect in your data.

Data: We will use the dataset that we used for exploratory analysis in Module 1. Load the mouse data.

```{r, class.source="codeblock",eval=TRUE}
library(clValid)
data("mouse")
mouse_exp = mouse[,c("M1","M2","M3","NC1","NC2","NC3")]
```


Step 1. Preparing Our Data

It is important to make sure that all the variables in your dataset are on the same scale to ensure they are comparable. So, let us check if that is the case with our dataset. To do that, we will first compute the means and variances of each variable using `apply()`.

```{r, class.source="codeblock",eval=TRUE}
apply(mouse_exp, 2, mean)
apply(mouse_exp, 2, var)
```

As you can see, the means and variances for all the six variables are almost the same and on the same scale, which is great!

However, keep in mind that, the variables need not always be on the same scale in other non-omics datasets. PCA is influenced by the magnitude of each variable. So, it is important to include a scaling step during data preparation. Ideally, it is great to have variables centered at zero for PCA because it makes comparing each principal component to the mean straightforward. Scaling can be done either using `scale()`. 
Step 2. Apply PCA

Since our variables are on the same scale, we can directly apply PCA using `prcomp()`.

```{r, class.source="codeblock",eval=TRUE}
pc_out <- prcomp(mouse_exp)
```

The output of `prcomp()` is a list. Examine the internal structure of `pc_out`.

```{r, class.source="codeblock",eval=TRUE}
str(pc_out)
```

The output of `prcomp()` contains five elements `sdev`, `rotation`, `center`, `scale` and `x`. Let us examine what each looks like. 

```{r, class.source="codeblock",eval=TRUE}
pc_out$sdev
```

`sddev` gives standard deviation

```{r, class.source="codeblock",eval=TRUE}
pc_out$rotation
```

After PCA, the observations are expressed in new axes and the principal component scores or loadings are provided in `pc_out$rotation`. Each column of `pc_out$rotation` contains the corresponding principal component loading vector.

We see that there are six distinct principal components, as indicated by column names of `pc_out$rotation`. 
```{r, class.source="codeblock",eval=TRUE}
pc_out$center
pc_out$scale
```

The `center` and `scale` elements correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA.

```{r, class.source="codeblock",eval=TRUE}
# See the principal components
dim(pc_out$x)
pc_out$x
```


Let’s now see the summary of the analysis using the `summary()` function!

```{r, class.source="codeblock",eval=TRUE}
summary(pc_out)
```

The first row gives the `Standard deviation` of each component, which is the same as the result of `pc_out$sdev`. 

The second row, `Proportion of Variance`, shows the percentage of explained variance, also obtained as variance/sum(variance) where variance is the square of sdev.

Compute variance
```{r, class.source="codeblock",eval=TRUE}
pc_out$sdev^2 / sum(pc_out$sdev^2)
```

From the second row you can see that the first principal component explains over 89.15% of the total variance (Note: multiply each number by 100 to get the percentages). 

The second principal component explains 8.9% of the variance, and the amount of variance explained reduces further down with each component. 

Finally, the last row, `Cumulative Proportion`, calculates the cumulative sum of the second row. 

Now, let's have some fun with visualising the results of PCA.

Step 3. Visualisation of PCA results

A. Scree plot

We can visualize the percentage of explained variance per principal component by using what is called a scree plot. We will call the `fviz_eig()` function of the factoextra package for the application. You may need to install the package.

```{r, class.source="codeblock",eval=TRUE}
library(factoextra)
fviz_eig(pc_out, 
         addlabels = TRUE)
```
X-axis shows the PCs and y-axis shows the percentage of variance explained that we saw above. Percentages are listed on top of the bars. It’s common to see that the first few principal components explain the major amount of variance.

Scree plot can also be used to decide the number of components to keep for rest of your analysis. One of the ways is using the elbow rule. This method is about looking for the “elbow” shape on the curve and retaining all components before the point where the curve flattens out. Here, the elbow appears to occur at the second principal component. 

Note that we will NOT remove any components for the current analysis since our goal is to understand how PCA can be used to identify batch effect in the data.

B. Scatter plot

After a PCA, the observations are expressed in principal component scores (as we saw above in `pc_out$rotation`). So, it is important to visualize the observations along the new axes (principal components) how observations have been transformed and to understand the relations in the dataset.

This can be achieved by drawing a scatterplot. To do so, first, we need to extract and the principal component scores in `pc_out$rotation`, and then we will store them in a data frame called PC.

```{r, class.source="codeblock",eval=TRUE}
PC <- as.data.frame(pc_out$rotation)
```

Plot the first two principal components as follows:

```{r, class.source="codeblock",eval=TRUE}
plot(x = PC$PC1,
     y = PC$PC2,
     pch = 19,
     xlab="PC1",
     ylab="PC2")
```
We see six points in two different groups. The points correspond to six samples. But, we don't know what group/condition they belong to.

That can be done by adding sample-related information to the data.frame `PC` (such as cell type, treatment type, batch they were processed etc) as new variables. Here we will add the sample names and the cell types. 

```{r, class.source="codeblock",eval=TRUE}
PC$sample <- factor(rownames(PC))
PC$cells <- factor(c(rep("mesoderm", 3), rep("neural_crest", 3)))
```

Plot the scatterplot again and now, colour the points by cell type. Then, add sample names and legend.

```{r, , class.source="codeblock",eval=TRUE, fig.width=7, fig.height=7}
plot(x = PC$PC1,
     y = PC$PC2,
     col = PC$cells,
     pch = 19,
     xlab="PC1",
     ylab="PC2")

text(x= PC$PC1,
     y = PC$PC2-0.02,
     labels = PC$sample)

legend("bottomright", 
       legend = levels(PC$cells), 
       col = seq_along(levels(PC$cells)), 
       pch = 19)
 
```
Samples from each cell type are closer together on the scatter plot. If the batch information is available, it can also be used to colour the scatterplot. Ideally, samples from different conditions should cluster together, irrespective of the batch they were processed in.

You can also plot other PCs such as PC2 vs PC3 by changing the x and y variables above. Another way to plot all PCs is using `pairs()`
```{r, class.source="codeblock",eval=TRUE}
pairs(PC[,1:6], 
      col=c("black","red")[PC$cells], 
      pch=16)
```


C. Biplot

Another useful plots to understand the results are biplots. We will use the `fviz_pca_biplot()` function of the factoextra package. We will set label="var" argument to label the variables.

```{r, class.source="codeblock",eval=TRUE}
fviz_pca_biplot(pc_out, label = "var")
```

The axes show the principal component scores, and the vectors are the loading vectors, whose components are in the magnitudes of the loadings. Vectors indicate that samples from each cell type are closer together. 

METHOD 2: t-Distributed Stochastic Neighbor Embedding (t-SNE)

t-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.

There are several packages that have implemented t-SNE. Here we are going to use  the package `tsne` and the function `tsne`. Let’s run the t-SNE algorithm on the mouse and generate a t-SNE plot.

```{r, class.source="codeblock",eval=TRUE}
library(tsne)
mouse_tsne = tsne(log(mouse_exp))

library(RColorBrewer)
my_cols_vec = brewer.pal("Set3",n = 9)
plot(mouse_tsne,
     pch=16, 
     col=my_cols_vec)
legend("topright",
        legend = levels(mouse$FC),
        col = my_cols_vec,
        pch = 19)
```
tsne function has a parameter called `perplexity` which determines how to balance attention to neighborhood vs global structure. Default value is 30 which was used above. Set perplexity to 10, 20, 50, 100 and rerun tsne. Then visualise each result.

```{r, class.source="codeblock",eval=TRUE}
mouse_tsne10 = tsne(log(mouse_exp),perplexity = 10)
mouse_tsne20 = tsne(log(mouse_exp),perplexity = 20)
mouse_tsne50 = tsne(log(mouse_exp),perplexity = 50)
mouse_tsne100 = tsne(log(mouse_exp),perplexity = 100)
```


```{r, fig.width=7, fig.height=7, class.source="codeblock",eval=TRUE}
par(mfrow=c(2,2))
plot(mouse_tsne10[,1],
     mouse_tsne10[,2],
     main = "Perplexity = 10",
     col = my_cols_vec,
     pch=16)
plot(mouse_tsne20[,1],
     mouse_tsne20[,2],
     main = "Perplexity = 20",
     col = my_cols_vec,
     pch=16)
plot(mouse_tsne50[,1],
     mouse_tsne50[,2],
     main = "Perplexity = 50",
     col = my_cols_vec,
     pch=16)
plot(mouse_tsne100[,1],
     mouse_tsne100[,2],
     main = "Perplexity = 100",
     col = my_cols_vec,
     pch=16)
```
Higher perplexity leads to higher spread in your data.

METHOD 3 - Uniform Manifold Approximation and Projection (UMAP)

UMAP is another dimension reduction method and it uses similar neighborhood approach as t-SNE except uses Riemannian geometry.

Here we are going to use  the package `umap` and the function `umap`. Let’s apply UMAP on the mouse and generate a UMAP plot.

```{r, class.source="codeblock",eval=TRUE}
## umap
library(umap)

mouse_umap = umap(log(mouse_exp))
str(mouse_umap)
```


```{r, class.source="codeblock",eval=TRUE}
par(mfrow=c(1,1))
plot(mouse_umap$layout[,1],
     mouse_umap$layout[,2],
     col = my_cols_vec, pch = 19)
legend("topright",
        legend = levels(mouse$FC),
        col = my_cols_vec,
        pch = 19)
```
## Exercise {-}

For your exercise, try the following:

* Return to your crabs data
* Compute the principle components (PCs) for the numeric columns 
* Plot these PCs and color them by species (“sp”) and sex 
* Now compute 2 t-SNE components for these data and color by species and sex
* Finally compute 2 UMAP components for these data and color by species and sex 
* Do any of these dimensionality reduction methods seem to segregate sex/species groups? 


<!--chapter:end:11-Module2_PCA.Rmd-->

# Module 2 exercise {-}

PCA
```{r, class.source="codeblock",eval=FALSE}
c_pcs = prcomp(crabs_meas)
```

Plot PC projections (embeddings). 
```{r, class.source="codeblock",eval=FALSE}
pairs(c_pcs$x, col = c("orchid","forestgreen")[factor(crabs$sp)])
pairs(c_pcs$x, col = c("orchid","forestgreen")[factor(crabs$sex)])
```

tSNE:

```{r, class.source="codeblock",eval=FALSE}
library(tsne)
c_tsne10 = tsne(crabs_meas,perplexity = 10)
c_tsne20 = tsne(crabs_meas,perplexity = 20)
c_tsne50 = tsne(crabs_meas,perplexity = 50)
c_tsne100 = tsne(crabs_meas,perplexity = 100)
```


sex_cols = c("orchid","forestgreen")[factor(crabs$sex)]

Color-code tSNE plot by species, try various perplexity levels:
```{r, class.source="codeblock",eval=FALSE}
species_cols = c("orchid","forestgreen")[factor(crabs$sp)]
par(mfrow=c(2,2))
plot(c_tsne10[,1],
     c_tsne10[,2],
     main = "Perplexity = 10",
     col = species_cols)

plot(c_tsne20[,1],
     c_tsne20[,2],
     main = "Perplexity = 20",
     col = species_cols)
plot(c_tsne50[,1],
     c_tsne50[,2],
     main = "Perplexity = 50",
     col = species_cols)
plot(c_tsne100[,1],
     c_tsne100[,2],
     main = "Perplexity = 100",
     col = species_cols)
```

Now do the same, but colour-code for sex:

```{r, class.source="codeblock",eval=FALSE}
sex_cols = c("orchid","forestgreen")[factor(crabs$sex)]
par(mfrow=c(2,2))
plot(c_tsne10[,1],
     c_tsne10[,2],
     main = "Perplexity = 10",
     col = sex_cols)
plot(c_tsne20[,1],
     c_tsne20[,2],
     main = "Perplexity = 20",
     col = sex_cols)
plot(c_tsne50[,1],
     c_tsne50[,2],
     main = "Perplexity = 50",
     col = sex_cols)
plot(c_tsne100[,1],
     c_tsne100[,2],
     main = "Perplexity = 100",
     col = sex_cols)
```


Run UMAP
```{r, class.source="codeblock",eval=FALSE}
library(umap)
c_umap <- umap(crabs_meas)
str(c_umap)

par(mfrow=c(1,2))
plot(c_umap$layout[,1],
     c_umap$layout[,2],
     col = species_cols, pch = 19, 
     main = "Colored by species")

plot(c_umap$layout[,1],
     c_umap$layout[,2],
     col = sex_cols, pch = 19, 
     main = "Colored by sex")
```

<!--chapter:end:12-Module2_Exercise.Rmd-->

# Module 3: Generalized Linear Models {-}

 In this module we're going to cover:

 * Reading data from files and evaluating missingness
 * Creating publication-quality plots with `ggplot2`
 * Fitting models with binary outcomes using generalized linear models


## Read data from files and explore variable distribution {-}
We're going to use a popular dataset for data analysis, pertaining to survival of passengers aboard the Titanic. 

Let's read in the data from a file:
```{r, class.source="codeblock",eval=TRUE}
dat <- read.delim(
    "data/titanic.csv",
    sep="," # indicate the column separator
    ) 
```

Examine the columns:
```{r, class.source="codeblock",eval=TRUE}
head(dat)
```

Some of the columns are categorical, use `table()` to look at the tallies:
Examine the columns:
```{r, class.source="codeblock",eval=TRUE}
table(dat$Survived)
table(dat$Pclass)
```

Use `summary()` to look at continuous-valued data:
```{r, class.source="codeblock",eval=TRUE}
summary(dat$Age)
```

Notice that there are 177 `NA` (missing) values for age.
Let's visualize the missing data more systematically.

## Explore missing data {-}

For this let's use a little script that converts a table into black and white squares to visualize missing data. For this, install the `plotrix` package

```{r, class.source="codeblock",eval=TRUE}
if (!requireNamespace("plotrix", quietly = TRUE)) install.packages("plotrix")

suppressMessages(require(plotrix))

#' show data missingness as a chequered matrix
#' 
#' @param x (matrix) data matrix.
#' @param outFile (char) path to file for printing graph
#' @param wd (numeric) width in inches
#' @param ht (numeric) height in inches
#' @return plots missingness matrix to file
#' @import plotrix
#' @export
plotMissMat <- function(x,xlab="columns",
		ylab="rows",border=NA) {
	
	x <- !is.na(x)
	class(x) <- "numeric"
	color2D.matplot(x,show.values=FALSE,axes=FALSE,
		cs1=c(1,0),cs2=c(1,0),cs3=c(1,0),border=border,
		cex=0.8,
		xlab=xlab,ylab=ylab)
}
```

Let's look at the missingness in the Titanic dataset. Missing data is shown as a white cell, and non-missing data is shown in black.

```{r, class.source="codeblock",eval=TRUE}
plotMissMat(dat)
```

We can see a column with many missing values. This is probably the "age" data. 
Let's count the number of missing values on a per-column level.

For this we combine `is.na()`, which returns a TRUE/FALSE value for NA values, and `colSums()` which adds up the TRUE values down the columns.

```{r count-missing, class.source="codeblock",eval=TRUE}
colSums(is.na(dat))
```
This confirms that `Age` is the only column with missing data.

Now let's explore the data using plots.

## Create plots with `ggplot2` to explore variable relationships {-}

`ggplot2` is a popular plotting package that uses an additive paradigm to build plots.  The [ggplot2 website]() is a wealth of reference information, so here we just touch on the basics to get you started. 

Anytime you need to generate a specific kind of plot, the website will most likely have documentation for how to achieve it.

Let's start by creating a scatterplot. For this let's load a dataset measuring statistics around quality of life in US states in the late 70's:

```{r, class.source="codeblock",eval=TRUE}
state.x77 <- as.data.frame(state.x77)
head(state.x77)
```

Create a base plot using the `ggplot` function. Then "add" a scatterplot to it. Notice that the plot has been assigned to a variable named `p`. 

This setup is standard for `ggplot2` and allows multiple visualizations to be applied to the same base plot.


```{r load-states-data, class.source="codeblock",eval=TRUE}
require(ggplot2)
p <- ggplot(state.x77, aes(Illiteracy,Income))

p <- p + geom_point() # scatter plot
p
```

Now let's add confidence intervals:
```{r states-geomsmooth, class.source="codeblock",eval=TRUE}
p + geom_smooth()
```


It looks like there is a negative relationship between illiteracy and income.
We can confirm this by looking at correlation:
```{r test-cor, class.source="codeblock",eval=TRUE}
x <- state.x77$Illiteracy
y <- state.x77$Income
cor.test(x,y)
cor.test(x,y)$p.value
```

Let's now examine categorical variables. 
In the titanic set, let's look at the fare paid based on passenger class:
```{r boxplot, class.source="codeblock",eval=TRUE}
p <- ggplot(dat)
p + geom_boxplot(
    aes(factor(Pclass), # make categorical
        Fare))
```

We can use barplots to examine counts and proportions.
Let's look at number of survivors, split by passenger class
```{r  barplot, class.source="codeblock",eval=TRUE}
p + geom_bar(
    aes(fill=factor(Survived), Pclass)
)
```

The plot above shows count data. Let's convert this to proportions. We can see that the fraction of non-survivors in "Class 3" is high.
```{r barplot2 , class.source="codeblock",eval=TRUE}
p + geom_bar(
    aes(fill=factor(Survived), Pclass),
    position = "fill"
)
```

How about males versus females?
```{r finish-titanic, class.source="codeblock",eval=TRUE}
p + geom_bar(
    aes(fill=factor(Survived), Sex),
    position = "fill"
)
```

## Fit binary response variable using `glm()` and logistic regression {-}

Let's fit a model to a binary outcome. For this we load a dataset that measures physiological variables in a cohort of Pima Indians.

```{r load-pima, class.source="codeblock",eval=TRUE}
require(mlbench)
data(PimaIndiansDiabetes2)
# type ?PimaIndiansDiabetes2 to learn more about the dataset.

dat <- PimaIndiansDiabetes2
head(dat)
```

Let's look at the impact of blood glucose levels on diabetes diagnosis. First let's make a scatterplot. 

Could there be a relationship?

```{r plot-diabetes-vs-glucose, class.source="codeblock",eval=TRUE}
p <- ggplot(dat, aes(glucose, factor(diabetes)))
p + geom_point()
```

Could this be fit with a linear model?

```{r, class.source="codeblock",eval=TRUE}
p <- ggplot(dat, aes(glucose, factor(diabetes)))
p + geom_point() + geom_smooth()
```

This is a situation where a logistic regression model would be an appropriate choice. We're going to use `glm()` to fit a model to these data:

```{r diabetes-model , class.source="codeblock",eval=TRUE}
mod <- glm(factor(diabetes)~glucose, 
    dat,
    family = "binomial" # set to model binary outcome
)
summary(mod)
```

Which factors explain a diabetes diagnosis?

What if we include a couple other factors?
```{r diabetes-model-expanded, class.source="codeblock",eval=TRUE}
mod <- glm(factor(diabetes)~ glucose + pregnant + age + pressure + triceps,
    dat,
    family = "binomial")
summary(mod)
```

```{block,type="rmd-caution"}
Note: This morning's session only intends to introduce you to fitting non-linear models. 

In practice you may need to do more work to test multiple models to ascertain best fits your data, using measures such as goodness-of-fit. You will also likely compute odds ratio (odds of increased Y per unit increase X), which is out of scope for the current tutorial. 

We strongly recommend that you learn these topics before applying these methods to your own data.
```

## Exercise {-}

Now let's apply the ideas above to a dataset for classifying a breast cell as being either benign or malignant. 

```{r bc-load, class.source="codeblock",eval=TRUE}
data(BreastCancer)
bc <- BreastCancer
for (k in 2:10) # altered for current lab
    bc[,k] <- as.numeric(bc[,k]) 
head(bc)
```

Learn more about the dataset:
```{r bc-explore, class.source="codeblock",eval=TRUE}
?BreastCancer
```


For your exercise, answer the following questions:

* Is there missing data?
* Which columns are 
* Use plots to explore the relationship between explanatory variables.
* Fit a logistic model to identify which factors explain cell size.

<!--chapter:end:21-Module3_LinearModels.Rmd-->

# Module 3 exercise {-}

```{r ex3-load, class.source="codeblock",eval=FALSE}
data(BreastCancer)
bc <- BreastCancer
for (k in 2:10) # altered for current lab
    bc[,k] <- as.numeric(bc[,k]) 
head(bc)
```

Explore missingness:
```{r ex3-miss1, class.source="codeblock",eval=FALSE}
suppressMessages(require(plotrix))

#' show data missingness as a chequered matrix
#' 
#' @param x (matrix) data matrix.
#' @param outFile (char) path to file for printing graph
#' @param wd (numeric) width in inches
#' @param ht (numeric) height in inches
#' @return plots missingness matrix to file
#' @import plotrix
#' @export
plotMissMat <- function(x,xlab="columns",
		ylab="rows",border=NA) {
	
	x <- !is.na(x)
	class(x) <- "numeric"
	color2D.matplot(x,show.values=FALSE,axes=FALSE,
		cs1=c(1,0),cs2=c(1,0),cs3=c(1,0),border=border,
		cex=0.8,
		xlab=xlab,ylab=ylab)
}
```

Explore missingness:
```{r ex3-miss2, class.source="codeblock",eval=FALSE}
plotMissMat(bc)
colSums(is.na(bc))
```

Plot relationship between variables
```{r ex3-plot, class.source="codeblock",eval=FALSE}
require(ggplot2)

ggplot(bc,aes(Cell.size,Normal.nucleoli)) + geom_point()
ggplot(bc) + geom_boxplot(aes(factor(Class), Normal.nucleoli))
ggplot(bc) + geom_boxplot(aes(factor(Class), Cl.thickness))
ggplot(bc) + geom_boxplot(aes(factor(Class), Bare.nuclei))
```

Fit a binary outcome model:
```{r ex3-glm, class.source="codeblock",eval=FALSE}
mod <- glm(
    Class ~ Cl.thickness + Bare.nuclei + Normal.nucleoli + Mitoses + Bl.cromatin, 
    bc,
    family="binomial")
summary(mod)
```

<!--chapter:end:22-Module3_Exercise.Rmd-->

# Module 4: Finding differentially expressed genes with RNA-seq {-}

We are going to look at genes differentially expressed between Luminal and Basal subtypes of breast cancer.

For this we are going to download data from The Cancer Genome Atlas [ADD LINK] readily available in BioConductor.

## Mini introduction to BioConductor {-}
BioConductor is the most popular software repository for genomics data analysis, and is worth exploring in detail.
This R-based ecosystem is open-source and community built. It contains software **LINK** packages but also **DATA** and **ANNOTATION** packages.


Examples of packages in BioConductor:

* Data package: The entire human genome sequence e.g., for the current GRCH38 build
* Data package: Multi-modal genomic data from The Cancer Genome Atlas for various tumour types
* Annotation package: Annotation for the Illumina DNA methylation array
* Software: `DEseq` to process RNAseq data, `minfi` to process DNA methylation array data, CHIPseeker for Chip-Seq data, etc., 

As we will see, BioConductor also has specialized *data structures* for bioinformatics operations.
Data structures are specialized, standardized formats for organizing, accessing, modifying and storing data.

Examples of BioConductor data structures:

* `GenomicRanges`: for working with genomic coordinates
* `SummarizedExperiment`: container for storing data and metadata about an experiment
* `MultiAssayExperiment`: container for experiments where multiple genomic assays were run on the same samples

BioConductor uses its own package manager to install packages. Instead of `install.packages()` we use, `BiocManager::install()`.

```{r, class.source="codeblock",eval=TRUE}
 if (!requireNamespace("curatedTCGAData", quietly = TRUE)) 
    BiocManager::install("curatedTCGAData")
```

## Fetch breast cancer data using `curatedTCGAData`  {-}

Load the package:
```{r, class.source="codeblock",eval=TRUE}
library(curatedTCGAData)
```

Let's take a look at the available data for breast cancer, without downloading anything (set `dry.run=TRUE`). 
Each row shows a data layer available. e.g., "*mRNA*" is transcriptomic data, "*CNA*" is chromosomal copy number aberration data, etc., 

```{r dge-brca-dryrun, class.source="codeblock",eval=TRUE}
curatedTCGAData(diseaseCode="BRCA", assays="*",dry.run=TRUE, version="2.0.1")
```

We want to get unprocessed gene expression read counts, so let's fetch the `BRCA_RNASeq2Gene-20160128` layer. You can use `?curatedTCGAData` to see a description of all data layers. 


```{r dge-fetchbrca, class.source="codeblock",eval=TRUE}
brca <- suppressMessages(
    curatedTCGAData(
        "BRCA",
        assays="RNASeqGene",
        dry.run=FALSE, 
        version="2.0.1")
)
```

This call returns a `MultiAssayExperiment` object. Recall that this is a container for storing multiple assays performed on the same set of samples. [See this tutorial](https://bioconductor.org/packages/release/bioc/vignettes/MultiAssayExperiment/inst/doc/QuickStartMultiAssay.html) to learn more.

Let's briefly explore the `brca` `MultiAssayExperiment` object.

```{r, class.source="codeblock",eval=TRUE}
brca
```

`assays()` returns a `list` with all -omic data associated with this object. Here we just have the one we downloaded. 

```{r, class.source="codeblock",eval=TRUE}
summary(assays(brca))
```
`names()` shows the datatypes in each slot of `assays()`:

```{r, class.source="codeblock",eval=TRUE}
names(assays(brca))
```
So gene expression in slot 1.

We can subset the data to see what it looks like. Let's look at just the first five measures, for the first 10 samples:

```{r dge-xprsubset, class.source="codeblock",eval=TRUE}
xpr <- assays(brca)[[1]]
head(xpr[1:10,1:5])
```

How many measures do we have?
```{r, class.source="codeblock",eval=TRUE}
nrow(xpr)
```

### Prepare data for differential expression analysis {-}

Process the expression values:
```{r dge-brca-prepare, class.source="codeblock",eval=TRUE}
cnames <- colnames(xpr)
hpos <- gregexpr("-",cnames)
tmp <- sapply(1:length(cnames),function(i) {substr(cnames[i],1,hpos[[i]][3]-1)})
idx <- !duplicated(tmp)

# subset values
tmp <- tmp[idx]
xpr <- xpr[,idx]
colnames(xpr) <- tmp
```

Patient metadata is contained in the `colData()` slot. Rows contain data for each patient and columns contain measures such as clinical characteristics:

```{r dge-phenolook, class.source="codeblock",eval=TRUE}
pheno <- colData(brca)
colnames(pheno)[1:20]
head(pheno[,1:5])
```

Let's confirm that samples are in the same order in the `xpr` matrix and in the metadata table `pheno`.

```{r, class.source="codeblock",eval=TRUE}
ids_in_both <- intersect(pheno$patientID, colnames(xpr))
pheno <- pheno[which(pheno$patientID %in% ids_in_both),]
xpr <- xpr[,which(colnames(xpr) %in% ids_in_both)]

if (all.equal(pheno$patientID, colnames(xpr))!=TRUE) {
    midx <- match(pheno$patientID, colnames(xpr))
    xpr <- xpr[,midx]
}
```

```{block,type="rmd-caution"}
Always check that samples are in the same order in the tables you are comparing, and use `match()` to reorder them if necessary.
If they are not in the same order, you are matching data to the wrong sample, and your results will be wrong.
```

This table has many columns. Let's just keep those related to tumour type according to the PAM50 classification.
What tumour types do we have in this dataset?

```{r dge-pheno-table, class.source="codeblock",eval=TRUE}
pheno <- pheno[,c("patientID","PAM50.mRNA")]
table(pheno$PAM50.mRNA)
```

Let's limit the comparison to `Luminal A` and `Basal` type tumours. 
```{r dge-phenomatch, class.source="codeblock",eval=TRUE}
idx <- which(pheno$PAM50.mRNA %in% c("Luminal A","Basal-like"))
pheno <- pheno[idx,]
xpr <- xpr[,idx] 

dim(pheno)
dim(xpr)
```

## Differential expression analysis with `edgeR` {-}

Now that the data are prepared, let's created a `DGEList` object (DGE stands for "Differential Gene Expression"). This object is what we will use for our differential expression analysis.

Note: Make phenotype of interest categorical. In R that means converting to a `factor` type with categorical `levels`. You can think of levels as ordinal representations (e.g., first level = 1, second = 2, etc., )

If `levels=` are not set, the default uses alphabetical order.  We recommend explicitly setting levels so that there are no assumptions.

Load the `edgeR` package:

```{r , class.source="codeblock",eval=TRUE}
suppressMessages(require(edgeR))
```

Let's create a `DGEList` object for the differential expression analysis.  Note that `group` must be a categorical variable (use `factor()` to convert it to one):

```{r dge-create, class.source="codeblock",eval=TRUE}
group <- factor(
        pheno$PAM50.mRNA,
        levels=c("Luminal A","Basal-like")
    )
dge <- DGEList(
    counts = xpr,
    group = group
    )
```

Remove low-count genes: Only keep genes with 100 or greater counts per million for at least two samples:

```{r dge-seecounts, class.source="codeblock",eval=TRUE}
(dge$counts[1:6,1:20])
```

Look at counts per million using `cpm`:

```{r dge-cpm, class.source="codeblock",eval=TRUE}
cpm(dge)[1:5,1:5]
```

This next line is a bit complex so let's unpack it:

*  We are using `cpm(dge)>100` as a logical test ("which genes have cpm > 100?"). 
*  For each gene, we want that test to be true for at least two samples. For this we use `rowSums()` to add up how many samples meet that criteria. 
  
```{r dge-libsize, class.source="codeblock",eval=TRUE}
dim(dge) #before 
tokeep <- rowSums(cpm(dge)>100) >= 2
dge <- dge[tokeep,,keep.lib.sizes = FALSE]
dim(dge) #after
```

Normalize the data:
```{r dge-calcnorm, class.source="codeblock",eval=TRUE}
dge <- calcNormFactors(dge)
```

Visualize the data:
```{r, class.source="codeblock",eval=TRUE}
plotMDS(
    dge, 
    col=as.numeric(dge$samples$group), 
    pch=16
)
legend(
    "bottomleft", 
    as.character(unique(dge$samples$group)),
    col=c(1,2), pch=16
    )
```

Let's create a model design to identify genes with a `group` effect:

```{r dge-model, class.source="codeblock",eval=TRUE}
group <- dge$samples$group
mod <- model.matrix(~group)
```

Estimate variation ("dispersion") for each gene:
```{r dge-disp, class.source="codeblock",eval=TRUE}
dge <- estimateDisp(dge, mod)
```

Call differentially expressed genes. 
Here we:
* fit a model for each gene, using `glmFit`
* we have built in an estimate of gene-wise dispersion to better identify treatment effect (or "contrast")
* for each gene, we run a likelihood ratio test which compares which model fits the data better: a null model (treatment effect = 0)
 or a full model (treatment effect is non-zero)

Note that `coef=2` fetches the effects for the treatment effect; `coef=1` would fetch effects of the intercept term.

```{r dge-diffex, class.source="codeblock",eval=TRUE}
fit <- glmFit(dge,mod)
diffEx <- glmLRT(fit, coef = 2) # get coefficients for group term
```

Look at the top 10 differentially expressed genes:
```{r dge-toptags, class.source="codeblock",eval=TRUE}
tt <- topTags(diffEx, n=10)
tt
```

For the next steps we're going to need stats on all the genes we've tested. So let's get those:
```{r, top-tags-full,class.source="codeblock",eval=TRUE}
tt <- as.data.frame(
    topTags(diffEx, n=nrow(dge)
    )
)
```

P-value histogram gives us a sense of the false positive rate.

For data without any signal we may expect a p-value histogram that is randomly uniformly distributed between 0 and 1. By chance, 5% of the time we expect p-values < 0.05. 

```{r, hist-random-pvalues, class.source="codeblock",eval=TRUE}
rnd_pval <- runif(nrow(tt)) 
hist(
    rnd_pval,
    n=100,
    main="Histogram for uniformly-distributed data"
)
abline(v=0.05,col="red")
```

By comparison, this is the pvalue histogram for our comparison of interest:
```{r, histogram-real-pvalues, class.source="codeblock",eval=TRUE}
hist(
    tt$PValue,
    n=100, 
    main="BRCA Luminal A vs Basal-Like: nominal pvalues"
)
abline(v=0.05,col="red")
```

A QQplot directly compares the pvalues from our statistical tests to the expected values from a random uniform distribution:
```{r, qqplot, class.source="codeblock",eval=TRUE}
qqplot(
    tt$PValue, 
    rnd_pval, 
    xlab="p-values from real data",
    ylab="X~U[0,1]",
    pch=16,cex=0.5
)

# x=y line as reference
abline(0,1,col="red")
```

Now let's call differentially expressed genes using the `decideTestDGE()` function and use `summary()` to see how many genes are upregulated (value `+1`), downregulated (value `-1`) and not called as changed (value `0`)
```{r, decide-tests-dge, class.source="codeblock",eval=TRUE}
diffEx2 <- decideTestsDGE(diffEx, 
    adjust.method="BH", 
    p.value=0.05
)
summary(diffEx2)
```

A **volcano plot** can help visualize effect magnitude - log2 fold-change or `log2FC` in the table ` against the corresponding p-value. Here we create a volcano plot, and colour-code upregulated genes in red, and downregulated genes in blue.

Note that we are combining two different tables, `tt` and `diffEx2` so we need to ensure the order is the same. Otherwise the colours will be in the wrong order *(try it for yourself!)*.

```{r, class.source="codeblock",eval=TRUE}
midx <- match(rownames(tt), rownames(diffEx2))
diffEx2 <- diffEx2[midx,]

cols <- rep("black",nrow(diffEx2))
cols[which(diffEx2>0 )]  <- "red"
cols[which(diffEx2<0)]  <- "blue"
# volcano plot
plot(tt$logFC,-log10(tt$PValue),pch=16,
    col=cols)
abline(v=0,lty=3)
```

Finally we can write our differential expression results out to file:
```{r, class.source="codeblock",eval=TRUE}
write.table(tt,file="diffEx.results.txt",
    sep="\t",
    col=TRUE,
    row=TRUE,
    quote=FALSE
)
```

## Exercise {-}

* Install the `yeastRNASeq` package from Bioconductor and `library` it into your environment
* Import the geneLevelData using: `data(yeastRNASeq)`
* Learn about this data and then put it through the same workflow we just did for the breast cancer: 
1. Filter genes with < 2 mean expression across all samples
2. Create a new expression set object with your filtered genes
3. Normalize and plot your data
4. Create a model matrix for analysis
5. Fit your model 
6. How many significantly up-regulated genes are there at the 5% FDR level? How many significantly down-regulated genes? How many in total
7. Create a volcano plot
8. Bonus: Create a histogram of p-values. Is there a signal?

Is there anything about the data that might make you question the results?

<!--chapter:end:31-Module4_RNAseq.Rmd-->

# Module 4 - exercises {-}

```{r, class.source="codeblock",eval=FALSE}
if (!requireNamespace("yeastRNASeq", quietly = TRUE)) 
  BiocManager::install("yeastRNASeq")

library(yeastRNASeq)

data(geneLevelData)
gene_keep <- rowMeans(geneLevelData) > 2 

# What genes pass this threshold?
filtered <- geneLevelData[gene_keep,] 
str(filtered)
```

```{r, class.source="codeblock",eval=FALSE}
#  as.matrix(filtered): the count data in the right class
# phenoData: The sample information

group <- factor(rep(c("Mut", "WT"),each=2), levels = c("WT","Mut")) 
y <- DGEList(as.matrix(filtered), 
           group = group)  

## matrix of experimental design 
mod = model.matrix(~group, y)


## Normalize data
y <- calcNormFactors(y, method = "upperquartile")
y <- estimateDisp(y, mod)

fit = glmFit(y, mod)
lrt = glmLRT(fit, coef = 2)
diffEx2 <- decideTestsDGE(lrt, 
    adjust.method="BH", 
    p.value=0.05
)
table(diffEx2)
```

```{r, class.source="codeblock",eval=FALSE}
DEGS = topTags(lrt, n=nrow(y))$table
```

```{r, class.source="codeblock",eval=FALSE}
## check out differentially expressed genes
head(DEGS)

DEGS_sig = DEGS[DEGS$FDR < 0.05,]
head(DEGS_sig)
dim(DEGS_sig)
```

```{r, class.source="codeblock",eval=FALSE}
midx <- match(rownames(DEGS), rownames(diffEx2))
diffEx2 <- diffEx2[midx,]

cols <- rep("black",nrow(diffEx2))
cols[which(diffEx2>0 )]  <- "red"
cols[which(diffEx2<0)]  <- "blue"
# volcano plot
plot(DEGS$logFC,-log10(DEGS$PValue),pch=16,
    col=cols)
abline(v=0,lty=3)
```


<!--chapter:end:32-Module4_Exercise.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:41-references.Rmd-->

